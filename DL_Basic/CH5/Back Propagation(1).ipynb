{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid 계층 \n",
    "# sigmoid 계층의 역전파는 순전파의 출력(y)만으로 계산 가능\n",
    "\n",
    "class Sigmoid: \n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "\n",
    "    def forward(self, x): # 순전파의 출력을 인스턴스 변수 out에 보관\n",
    "        out = sigmoid(x)\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout): # 역전파 때 순전파 출력 out에 보관한거 사용\n",
    "        dx = dout * (1.0 - self.out) * self.out\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망의 순전파 → 가중치 신호의 총합 계산 때문에 행렬의 곱 사용 [넘파이에선 np.dot()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2, 3)\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "# 각각 형상이 다차원 배열\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.rand(2)    # 입력\n",
    "w = np.random.rand(2, 3) # 가중치\n",
    "b = np.random.rand(3)    # 편향\n",
    "\n",
    "print(x.shape)\n",
    "print(w.shape)\n",
    "print(b.shape)\n",
    "\n",
    "y = np.dot(x, w) + b # 뉴런의 가중치 합\n",
    "\n",
    "# 신경망 순전파의 흐름 → y를 활성화 함수로 변환하여 다음 층으로 전파하는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • 행렬 곱의 계산\n",
    ": 대응하는 차원의 원소 수를 일치시키는게 핵심"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• x와 w의 곱은 대응하는 차원의 원소 수를 일치시켜야 한다. \n",
    "\n",
    "• 행렬의 형상이 (2, 3)처럼 표시하는 이유 → 넘파이의 shape 함수 출력과 형태 통일하기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0]\n",
      " [10 10 10]]\n",
      "[[ 1  2  3]\n",
      " [11 12 13]]\n"
     ]
    }
   ],
   "source": [
    "# 배치용 Affine 계층\n",
    "\n",
    "x_dot_w = np.array([[0, 0, 0],[10, 10, 10]])\n",
    "b = np.array([1, 2, 3])\n",
    "\n",
    "print(x_dot_w)\n",
    "\n",
    "print(x_dot_w + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "# 역전파는 각 데이터의 역전파 값이 편향의 원소에 모여야 한다. \n",
    "\n",
    "dy = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(dy)\n",
    "\n",
    "db = np.sum(dy, axis=0) # 0번째 축에 대해 axis=0의 총합을 구하는 것\n",
    "print(db)\n",
    "\n",
    "# 데이터가 2개(n = 2)\n",
    "# 편향의 역전파는 두 데이터에 대한 미분을 데이터마다 더해서 구한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine 구현 \n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        \n",
    "        self.x = None\n",
    "        self.original_x_shape = None \n",
    "        \n",
    "        # 가중치와 편향 매개변수의 미분\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 텐서 대응\n",
    "        self.original_x_shape = x.shape\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        self.x = x\n",
    "\n",
    "        out = np.dot(self.x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        \n",
    "        dx = dx.reshape(*self.original_x_shape)  # 입력 데이터 모양 변경(텐서 대응)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 소프트맥스 \n",
    ": 입력 값을 정규화하여 출력\n",
    "\n",
    "####\n",
    "∙ 입력 값을 정규화(출력의 합이 1이 되게)하여 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 신경망 학습의 목적\n",
    ": 신경망의 출력(softmax)이 정답 레이블과 가까워지도록 가중치 매개변수의 값 조정하는 것\n",
    "\n",
    ": 신경망의 출력과 정답 레이블 오차를 효율적으로 앞 계층에 전달"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax_with_Loss\n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None # 손실함수\n",
    "        self.y = None    # softmax의 출력\n",
    "        self.t = None    # 정답 레이블(원-핫 인코딩 형태)\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.t.size == self.y.size: # 정답 레이블이 원-핫 인코딩 형태일 때\n",
    "            dx = (self.y - self.t) / batch_size\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1\n",
    "            dx = dx / batch_size\n",
    "        \n",
    "        return dx\n",
    "\n",
    "# 역전파 때는 전파하는 값을 배치의 수(batch_size)로 나눠서 데이터 1개당 오차를 앞 계층으로 전파"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 학습 전체 그림\n",
    "\n",
    "- 전제\n",
    ": 신경망엔 적응 가능한 가중치, 편향이 있다. 총 4단계로 수행\n",
    "\n",
    "1. 미니배치 \n",
    ": 훈련 데이터 중 일부를 무작위로 가져옴 / 미니배치의 손실 함수 값 줄이는 것이 목표\n",
    "###\n",
    "2. 기울기 산출\n",
    ": 미니배치의 손실 함수 값 줄이기 위해 각 가중치 매개변수의 기울기 구하기.\n",
    ": 기울기는 손실 함수의 값을 가장 작게 하는 방향 제시\n",
    "###\n",
    "3. 매개변수 갱신 \n",
    ": 가중치 매개변수 기울기 방향으로 아주 조금 갱신\n",
    "###\n",
    "4. 반복 \n",
    ": 1~3단계 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞 장과 다른 부분은 계층을 사용한다. \n",
    "# 인식 결과 얻는 처리 (predict()), 기울기 구하는 처리(gradient())\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('/Users/krc/Downloads/deep-learning-from-scratch-master')\n",
    "import numpy as np\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "from collections import OrderedDict\n",
    " \n",
    " \n",
    "class TwoLayerNet:\n",
    " \n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    " \n",
    "        # 계층 생성\n",
    "        # OrderedDict = 순서가 있는 딕셔너리, 순서 기억\n",
    "        # 순전파 때는 계층을 추가한 순서대로 / 역전파 때는 계층 반대 순서로 호출\n",
    "        self.layers = OrderedDict() # 순서가 있는 딕셔너리 \n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    " \n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x: 입력데이터, t : 정답레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x: 입력데이터, t : 정답레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward, 순전파\n",
    "        self.loss(x, t)\n",
    " \n",
    "        # backward, 역전파\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    " \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    " \n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기울기 구하는 방법\n",
    "1. 수치 미분을 써서 구하는 방법\n",
    "\n",
    "2. 해석적으로 수식을 풀어 구하는 방법  → 오차역전파법 이용하여 매개변수가 많아도 효율적으로 계산 \n",
    "\n",
    "###\n",
    "• 기울기 확인 \n",
    ": 두 방식으로 구한 기울기가 일치함 (거의 같음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1:1.8440637058490597e-06\n",
      "b1:1.5782359941071237e-05\n",
      "W2:4.784549472707054e-09\n",
      "b2:1.4006088839835228e-07\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append('/Users/krc/Downloads/deep-learning-from-scratch-master/ch05')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "x_batch = x_train[:3]\n",
    "t_batch = t_train[:3]\n",
    "\n",
    "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
    "grad_backprop = network.gradient(x_batch, t_batch)\n",
    "\n",
    "# 각 가중치의 절대 오차의 평균을 구한다.\n",
    "for key in grad_numerical.keys():\n",
    "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
    "    print(key + \":\" + str(diff))\n",
    "\n",
    "# 수치 미분으로 구한 기울기와 오차역전파법으로 구한 기울기의 오차 확인\n",
    "# 각 가중치 매개변수 차이의 절댓값 구하고 평균한 값이 오차가 된다. \n",
    "# → 수치 미분과 오차역전파법으로 구한 기울기의 차이가 매우 작다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13926666666666668 0.1425\n",
      "0.90425 0.9096\n",
      "0.9218833333333334 0.9248\n",
      "0.9317833333333333 0.9314\n",
      "0.9435166666666667 0.9424\n",
      "0.9502166666666667 0.9499\n",
      "0.955 0.9523\n",
      "0.9572166666666667 0.9544\n",
      "0.9639333333333333 0.9578\n",
      "0.9659 0.96\n",
      "0.9689833333333333 0.963\n",
      "0.9713833333333334 0.9637\n",
      "0.9743833333333334 0.9647\n",
      "0.97505 0.9671\n",
      "0.9748833333333333 0.9658\n",
      "0.9766666666666667 0.9675\n",
      "0.9785333333333334 0.9681\n"
     ]
    }
   ],
   "source": [
    "# 오차역전파법을 사용한 신경망 학습 \n",
    "# 기울기를 오차역전파법으로 구한다는 점\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('/Users/krc/Downloads/deep-learning-from-scratch-master/ch05')\n",
    "\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 10000\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
    "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
    "    \n",
    "    # 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(train_acc, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5장 정리\n",
    "\n",
    "#### 계산 그래프를 이용하여 신경망의 동작과 오차역전파법 설명 → 처리 과정을 계층이라는 단위로 구현\n",
    "\n",
    "ex) ReLU 계층, Softmax-with-Loss 계층, Affine 계층, Softmax 계층 등 \n",
    "###\n",
    "- 모든 계층에서 forward, backward 메서드 구현\n",
    "###\n",
    "- 전자는 데이터를 순방향 전파, 후자는 역방향으로 전파  → 가중치 매개변수의 기울기 효율적으로 구함\n",
    "\n",
    "### ▶︎ 이처럼 동작을 계층으로 모듈화한 덕분에, 신경망 계층을 자유롭게 조합하여 원하는 신경망 쉽게 만들 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • 이번 장에서 배운 내용\n",
    "\n",
    "1. 계산 그래프를 이용하면 계산 과정을 시각적으로 파악할 수 있다.\n",
    "\n",
    "2. 계산 그래프 노드는 국소적 계산으로 구성\n",
    "\n",
    "3. 순전파는 통상 계산 수행 / 역전파로는 각 노드의 미분 구할 수 있다. \n",
    "\n",
    "4. 신경망의 구성 요소를 계층으로 구현하여 기울기 효율적으로 계산 (오차역전파법)\n",
    "\n",
    "5. 수치 미분과 오차역전파법 결과 비교하연 오차역전파법의 구현에 잘못 없는지 확인 가능"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4685cf8fad66a3b5e1d83fe36d59de227ad64c6532a47c7a0b6e8e6da20ed48b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('krc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
