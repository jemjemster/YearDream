{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오버피팅 \n",
    "\n",
    "- 매개변수가 많고 표현력이 높은 모델\n",
    "\n",
    "- 훈련 데이터가 적다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/krc/Downloads/deep-learning-from-scratch-master')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.optimizer import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 일부러 오버피팅 시킴\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, train acc:0.09, test acc:0.093\n",
      "epoch:1, train acc:0.1, test acc:0.1081\n",
      "epoch:2, train acc:0.12666666666666668, test acc:0.1318\n",
      "epoch:3, train acc:0.18333333333333332, test acc:0.1585\n",
      "epoch:4, train acc:0.22, test acc:0.1833\n",
      "epoch:5, train acc:0.23333333333333334, test acc:0.1991\n",
      "epoch:6, train acc:0.29, test acc:0.2218\n",
      "epoch:7, train acc:0.35333333333333333, test acc:0.2435\n",
      "epoch:8, train acc:0.37666666666666665, test acc:0.257\n",
      "epoch:9, train acc:0.39666666666666667, test acc:0.2732\n",
      "epoch:10, train acc:0.4266666666666667, test acc:0.2931\n",
      "epoch:11, train acc:0.4266666666666667, test acc:0.3031\n",
      "epoch:12, train acc:0.42, test acc:0.3141\n",
      "epoch:13, train acc:0.41, test acc:0.3237\n",
      "epoch:14, train acc:0.4266666666666667, test acc:0.3373\n",
      "epoch:15, train acc:0.44333333333333336, test acc:0.3552\n",
      "epoch:16, train acc:0.4266666666666667, test acc:0.3618\n",
      "epoch:17, train acc:0.45, test acc:0.3725\n",
      "epoch:18, train acc:0.45, test acc:0.3802\n",
      "epoch:19, train acc:0.46, test acc:0.3907\n",
      "epoch:20, train acc:0.47333333333333333, test acc:0.3971\n",
      "epoch:21, train acc:0.4866666666666667, test acc:0.4092\n",
      "epoch:22, train acc:0.5, test acc:0.4167\n",
      "epoch:23, train acc:0.5133333333333333, test acc:0.4255\n",
      "epoch:24, train acc:0.5466666666666666, test acc:0.4353\n",
      "epoch:25, train acc:0.5333333333333333, test acc:0.4376\n",
      "epoch:26, train acc:0.5166666666666667, test acc:0.4314\n",
      "epoch:27, train acc:0.5366666666666666, test acc:0.4368\n",
      "epoch:28, train acc:0.55, test acc:0.4501\n",
      "epoch:29, train acc:0.54, test acc:0.4497\n",
      "epoch:30, train acc:0.5433333333333333, test acc:0.4455\n",
      "epoch:31, train acc:0.5366666666666666, test acc:0.4521\n",
      "epoch:32, train acc:0.54, test acc:0.4543\n",
      "epoch:33, train acc:0.5433333333333333, test acc:0.4581\n",
      "epoch:34, train acc:0.58, test acc:0.4829\n",
      "epoch:35, train acc:0.5633333333333334, test acc:0.4818\n",
      "epoch:36, train acc:0.5933333333333334, test acc:0.4913\n",
      "epoch:37, train acc:0.6, test acc:0.4999\n",
      "epoch:38, train acc:0.58, test acc:0.4855\n",
      "epoch:39, train acc:0.6133333333333333, test acc:0.5124\n",
      "epoch:40, train acc:0.6066666666666667, test acc:0.5101\n",
      "epoch:41, train acc:0.6033333333333334, test acc:0.4973\n",
      "epoch:42, train acc:0.5966666666666667, test acc:0.5061\n",
      "epoch:43, train acc:0.6233333333333333, test acc:0.514\n",
      "epoch:44, train acc:0.6233333333333333, test acc:0.5341\n",
      "epoch:45, train acc:0.6433333333333333, test acc:0.535\n",
      "epoch:46, train acc:0.63, test acc:0.5314\n",
      "epoch:47, train acc:0.6666666666666666, test acc:0.5536\n",
      "epoch:48, train acc:0.68, test acc:0.5636\n",
      "epoch:49, train acc:0.6933333333333334, test acc:0.5824\n",
      "epoch:50, train acc:0.6933333333333334, test acc:0.571\n",
      "epoch:51, train acc:0.6866666666666666, test acc:0.5814\n",
      "epoch:52, train acc:0.69, test acc:0.5718\n",
      "epoch:53, train acc:0.6966666666666667, test acc:0.588\n",
      "epoch:54, train acc:0.73, test acc:0.6092\n",
      "epoch:55, train acc:0.73, test acc:0.6014\n",
      "epoch:56, train acc:0.7533333333333333, test acc:0.6135\n",
      "epoch:57, train acc:0.7533333333333333, test acc:0.6139\n",
      "epoch:58, train acc:0.76, test acc:0.6186\n",
      "epoch:59, train acc:0.78, test acc:0.6264\n",
      "epoch:60, train acc:0.79, test acc:0.6272\n",
      "epoch:61, train acc:0.7766666666666666, test acc:0.6256\n",
      "epoch:62, train acc:0.79, test acc:0.631\n",
      "epoch:63, train acc:0.78, test acc:0.6307\n",
      "epoch:64, train acc:0.7966666666666666, test acc:0.6386\n",
      "epoch:65, train acc:0.8133333333333334, test acc:0.655\n",
      "epoch:66, train acc:0.8133333333333334, test acc:0.6528\n",
      "epoch:67, train acc:0.8266666666666667, test acc:0.6649\n",
      "epoch:68, train acc:0.8033333333333333, test acc:0.6551\n",
      "epoch:69, train acc:0.8033333333333333, test acc:0.6608\n",
      "epoch:70, train acc:0.8266666666666667, test acc:0.6689\n",
      "epoch:71, train acc:0.84, test acc:0.6699\n",
      "epoch:72, train acc:0.83, test acc:0.6733\n",
      "epoch:73, train acc:0.8433333333333334, test acc:0.6819\n",
      "epoch:74, train acc:0.8333333333333334, test acc:0.6784\n",
      "epoch:75, train acc:0.83, test acc:0.6823\n",
      "epoch:76, train acc:0.8466666666666667, test acc:0.686\n",
      "epoch:77, train acc:0.85, test acc:0.6879\n",
      "epoch:78, train acc:0.8533333333333334, test acc:0.6935\n",
      "epoch:79, train acc:0.86, test acc:0.697\n",
      "epoch:80, train acc:0.8433333333333334, test acc:0.6929\n",
      "epoch:81, train acc:0.8533333333333334, test acc:0.6837\n",
      "epoch:82, train acc:0.85, test acc:0.7015\n",
      "epoch:83, train acc:0.8533333333333334, test acc:0.6968\n",
      "epoch:84, train acc:0.8666666666666667, test acc:0.6983\n",
      "epoch:85, train acc:0.8666666666666667, test acc:0.7\n",
      "epoch:86, train acc:0.88, test acc:0.7063\n",
      "epoch:87, train acc:0.8766666666666667, test acc:0.7088\n",
      "epoch:88, train acc:0.8733333333333333, test acc:0.7143\n",
      "epoch:89, train acc:0.8733333333333333, test acc:0.7084\n",
      "epoch:90, train acc:0.8566666666666667, test acc:0.7071\n",
      "epoch:91, train acc:0.8766666666666667, test acc:0.71\n",
      "epoch:92, train acc:0.87, test acc:0.7178\n",
      "epoch:93, train acc:0.8666666666666667, test acc:0.7055\n",
      "epoch:94, train acc:0.8733333333333333, test acc:0.7137\n",
      "epoch:95, train acc:0.8833333333333333, test acc:0.7151\n",
      "epoch:96, train acc:0.8766666666666667, test acc:0.7209\n",
      "epoch:97, train acc:0.8766666666666667, test acc:0.726\n",
      "epoch:98, train acc:0.89, test acc:0.724\n",
      "epoch:99, train acc:0.8733333333333333, test acc:0.7153\n",
      "epoch:100, train acc:0.8766666666666667, test acc:0.7232\n",
      "epoch:101, train acc:0.8766666666666667, test acc:0.7203\n",
      "epoch:102, train acc:0.89, test acc:0.7274\n",
      "epoch:103, train acc:0.8933333333333333, test acc:0.7313\n",
      "epoch:104, train acc:0.8866666666666667, test acc:0.7288\n",
      "epoch:105, train acc:0.8866666666666667, test acc:0.7265\n",
      "epoch:106, train acc:0.88, test acc:0.7296\n",
      "epoch:107, train acc:0.8866666666666667, test acc:0.7223\n",
      "epoch:108, train acc:0.8866666666666667, test acc:0.7257\n",
      "epoch:109, train acc:0.8933333333333333, test acc:0.7242\n",
      "epoch:110, train acc:0.8933333333333333, test acc:0.7264\n",
      "epoch:111, train acc:0.8966666666666666, test acc:0.7367\n",
      "epoch:112, train acc:0.8933333333333333, test acc:0.731\n",
      "epoch:113, train acc:0.8966666666666666, test acc:0.7357\n",
      "epoch:114, train acc:0.88, test acc:0.7278\n",
      "epoch:115, train acc:0.8933333333333333, test acc:0.7432\n",
      "epoch:116, train acc:0.8933333333333333, test acc:0.7302\n",
      "epoch:117, train acc:0.8966666666666666, test acc:0.7457\n",
      "epoch:118, train acc:0.8966666666666666, test acc:0.7382\n",
      "epoch:119, train acc:0.89, test acc:0.7328\n",
      "epoch:120, train acc:0.89, test acc:0.7308\n",
      "epoch:121, train acc:0.91, test acc:0.7424\n",
      "epoch:122, train acc:0.9133333333333333, test acc:0.7456\n",
      "epoch:123, train acc:0.8966666666666666, test acc:0.7376\n",
      "epoch:124, train acc:0.8966666666666666, test acc:0.74\n",
      "epoch:125, train acc:0.8833333333333333, test acc:0.7342\n",
      "epoch:126, train acc:0.9033333333333333, test acc:0.7408\n",
      "epoch:127, train acc:0.8933333333333333, test acc:0.7301\n",
      "epoch:128, train acc:0.9, test acc:0.7291\n",
      "epoch:129, train acc:0.8933333333333333, test acc:0.7405\n",
      "epoch:130, train acc:0.9133333333333333, test acc:0.738\n",
      "epoch:131, train acc:0.9, test acc:0.7404\n",
      "epoch:132, train acc:0.89, test acc:0.7405\n",
      "epoch:133, train acc:0.9066666666666666, test acc:0.7439\n",
      "epoch:134, train acc:0.9, test acc:0.7422\n",
      "epoch:135, train acc:0.9033333333333333, test acc:0.7463\n",
      "epoch:136, train acc:0.9, test acc:0.7539\n",
      "epoch:137, train acc:0.91, test acc:0.7439\n",
      "epoch:138, train acc:0.9, test acc:0.7428\n",
      "epoch:139, train acc:0.91, test acc:0.7382\n",
      "epoch:140, train acc:0.8966666666666666, test acc:0.7481\n",
      "epoch:141, train acc:0.91, test acc:0.7497\n",
      "epoch:142, train acc:0.8933333333333333, test acc:0.7438\n",
      "epoch:143, train acc:0.9066666666666666, test acc:0.7493\n",
      "epoch:144, train acc:0.9066666666666666, test acc:0.7501\n",
      "epoch:145, train acc:0.9133333333333333, test acc:0.7466\n",
      "epoch:146, train acc:0.8966666666666666, test acc:0.743\n",
      "epoch:147, train acc:0.91, test acc:0.7524\n",
      "epoch:148, train acc:0.9133333333333333, test acc:0.7469\n",
      "epoch:149, train acc:0.9066666666666666, test acc:0.7437\n",
      "epoch:150, train acc:0.9, test acc:0.7482\n",
      "epoch:151, train acc:0.9066666666666666, test acc:0.7501\n",
      "epoch:152, train acc:0.9166666666666666, test acc:0.7459\n",
      "epoch:153, train acc:0.9066666666666666, test acc:0.7472\n",
      "epoch:154, train acc:0.92, test acc:0.7499\n",
      "epoch:155, train acc:0.92, test acc:0.7408\n",
      "epoch:156, train acc:0.92, test acc:0.7427\n",
      "epoch:157, train acc:0.9133333333333333, test acc:0.742\n",
      "epoch:158, train acc:0.91, test acc:0.7546\n",
      "epoch:159, train acc:0.91, test acc:0.7514\n",
      "epoch:160, train acc:0.92, test acc:0.7488\n",
      "epoch:161, train acc:0.9233333333333333, test acc:0.7551\n",
      "epoch:162, train acc:0.92, test acc:0.7542\n",
      "epoch:163, train acc:0.9233333333333333, test acc:0.7514\n",
      "epoch:164, train acc:0.9066666666666666, test acc:0.7535\n",
      "epoch:165, train acc:0.91, test acc:0.748\n",
      "epoch:166, train acc:0.9233333333333333, test acc:0.7461\n",
      "epoch:167, train acc:0.9166666666666666, test acc:0.7529\n",
      "epoch:168, train acc:0.9166666666666666, test acc:0.7521\n",
      "epoch:169, train acc:0.9166666666666666, test acc:0.751\n",
      "epoch:170, train acc:0.9166666666666666, test acc:0.7509\n",
      "epoch:171, train acc:0.9066666666666666, test acc:0.7429\n",
      "epoch:172, train acc:0.9133333333333333, test acc:0.75\n",
      "epoch:173, train acc:0.9166666666666666, test acc:0.7478\n",
      "epoch:174, train acc:0.9133333333333333, test acc:0.7498\n",
      "epoch:175, train acc:0.92, test acc:0.7489\n",
      "epoch:176, train acc:0.93, test acc:0.7494\n",
      "epoch:177, train acc:0.9266666666666666, test acc:0.755\n",
      "epoch:178, train acc:0.9233333333333333, test acc:0.7543\n",
      "epoch:179, train acc:0.9266666666666666, test acc:0.7571\n",
      "epoch:180, train acc:0.9166666666666666, test acc:0.7551\n",
      "epoch:181, train acc:0.9333333333333333, test acc:0.7534\n",
      "epoch:182, train acc:0.9266666666666666, test acc:0.7515\n",
      "epoch:183, train acc:0.9266666666666666, test acc:0.756\n",
      "epoch:184, train acc:0.9266666666666666, test acc:0.7549\n",
      "epoch:185, train acc:0.93, test acc:0.7572\n",
      "epoch:186, train acc:0.9266666666666666, test acc:0.7531\n",
      "epoch:187, train acc:0.9266666666666666, test acc:0.7573\n",
      "epoch:188, train acc:0.93, test acc:0.7556\n",
      "epoch:189, train acc:0.92, test acc:0.7529\n",
      "epoch:190, train acc:0.9233333333333333, test acc:0.754\n",
      "epoch:191, train acc:0.9233333333333333, test acc:0.7547\n",
      "epoch:192, train acc:0.92, test acc:0.7513\n",
      "epoch:193, train acc:0.9266666666666666, test acc:0.7535\n",
      "epoch:194, train acc:0.91, test acc:0.7439\n",
      "epoch:195, train acc:0.93, test acc:0.7532\n",
      "epoch:196, train acc:0.9233333333333333, test acc:0.7497\n",
      "epoch:197, train acc:0.9266666666666666, test acc:0.7522\n",
      "epoch:198, train acc:0.92, test acc:0.7543\n",
      "epoch:199, train acc:0.9333333333333333, test acc:0.7559\n",
      "epoch:200, train acc:0.93, test acc:0.7485\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# weight decay（가중치 감쇠） 설정 =======================\n",
    "#weight_decay_lambda = 0 # weight decay를 사용하지 않을 경우\n",
    "weight_decay_lambda = 0.1\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100], output_size=10,\n",
    "                        weight_decay_lambda = weight_decay_lambda)\n",
    "optimizer = SGD(lr=0.01) # 학습률이 0.01인 SGD로 매개변수 갱신\n",
    "\n",
    "max_epochs = 201\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "epoch_cnt = 0\n",
    "\n",
    "for i in range(1000000000):\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "\n",
    "    grads = network.gradient(x_batch, t_batch)\n",
    "    optimizer.update(network.params, grads)\n",
    "\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        print(\"epoch:\" + str(epoch_cnt) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc))\n",
    "\n",
    "        epoch_cnt += 1\n",
    "        if epoch_cnt >= max_epochs:\n",
    "            break\n",
    "\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# 정확도가 크게 벌어지는건 훈련 데이터에만 적응해버린 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 가중치 감소 \n",
    "\n",
    ": 학습 과정에서 큰 가중치에 대해서는 상응하는 큰 패널티를 부과하여 오버피팅 억제하는 방법\n",
    "####\n",
    "→ 오버피팅은 가중치 매개변수 값이 커서 발생하는 경우가 많다.\n",
    "\n",
    "\n",
    "#####\n",
    "▶︎ 신경망 학습의 목적 \n",
    "\n",
    ": 손실 함수의 값  줄이는 것\n",
    "\n",
    "\n",
    "#####\n",
    "• 람다는 정규화의 세기 조절하는 하이퍼파라미터 \n",
    "\n",
    "→ 크게 설정할수록 큰  가중치에 대한 페널티가 커진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('/Users/krc/Downloads/deep-learning-from-scratch-master')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class MultiLayerNet:\n",
    "    \"\"\"완전연결 다층 신경망\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    weight_decay_lambda : 가중치 감소(L2 법칙)의 세기\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size_list, output_size,\n",
    "                 activation='relu', weight_init_std='relu', weight_decay_lambda=0):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size_list = hidden_size_list\n",
    "        self.hidden_layer_num = len(hidden_size_list)\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.params = {}\n",
    "\n",
    "        # 가중치 초기화\n",
    "        self.__init_weight(weight_init_std)\n",
    "\n",
    "        # 계층 생성\n",
    "        activation_layer = {'sigmoid': Sigmoid, 'relu': Relu}\n",
    "        self.layers = OrderedDict()\n",
    "        for idx in range(1, self.hidden_layer_num+1):\n",
    "            self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "                                                      self.params['b' + str(idx)])\n",
    "            self.layers['Activation_function' + str(idx)] = activation_layer[activation]()\n",
    "\n",
    "        idx = self.hidden_layer_num + 1\n",
    "        self.layers['Affine' + str(idx)] = Affine(self.params['W' + str(idx)],\n",
    "            self.params['b' + str(idx)])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def __init_weight(self, weight_init_std):\n",
    "        \"\"\"가중치 초기화\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "            'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "            'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "        \"\"\"\n",
    "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
    "        for idx in range(1, len(all_size_list)):\n",
    "            scale = weight_init_std\n",
    "            if str(weight_init_std).lower() in ('relu', 'he'):\n",
    "                scale = np.sqrt(2.0 / all_size_list[idx - 1])  # ReLU를 사용할 때의 권장 초깃값\n",
    "            elif str(weight_init_std).lower() in ('sigmoid', 'xavier'):\n",
    "                scale = np.sqrt(1.0 / all_size_list[idx - 1])  # sigmoid를 사용할 때의 권장 초깃값\n",
    "            self.params['W' + str(idx)] = scale * np.random.randn(all_size_list[idx-1], all_size_list[idx])\n",
    "            self.params['b' + str(idx)] = np.zeros(all_size_list[idx])\n",
    "\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블 \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        손실 함수의 값\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "\n",
    "        weight_decay = 0\n",
    "        for idx in range(1, self.hidden_layer_num + 2):\n",
    "            W = self.params['W' + str(idx)]\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W ** 2)\n",
    "\n",
    "        return self.last_layer.forward(y, t) + weight_decay\n",
    "\n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "\n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "\n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(수치 미분).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = numerical_gradient(loss_W, self.params['W' + str(idx)])\n",
    "            grads['b' + str(idx)] = numerical_gradient(loss_W, self.params['b' + str(idx)])\n",
    "\n",
    "        return grads\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 딕셔너리(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        for idx in range(1, self.hidden_layer_num+2):\n",
    "            grads['W' + str(idx)] = self.layers['Affine' + str(idx)].dW + self.weight_decay_lambda * self.layers['Affine' + str(idx)].W\n",
    "            grads['b' + str(idx)] = self.layers['Affine' + str(idx)].db\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 가중치 감소는 간단하게 구현할 수  있고, 어느 정도 지나친 학습을 억제할 수  있다. \n",
    "\n",
    "- 신경망 모델이 복잡해지면 가중치 감소만으로는 대응이 어려워진다. 이럴 때  드롭아웃 이용\n",
    "\n",
    "### 드롭아웃\n",
    "\n",
    ": 뉴런을 임의로 삭제하면서 학습하는 방법\n",
    "\n",
    ": 훈련 때  은닉층의 뉴런을 무작위로 골라 삭제 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout:\n",
    "    def __init__(self,dropout_ratio = 0.5):\n",
    "        self.dropout_ratio = dropout_ratio\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_ratio\n",
    "            return x * self.mask\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_ratio)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2947637863658423\n",
      "=== epoch:1, train acc:0.1, test acc:0.0806 ===\n",
      "train loss:2.3101138206637053\n",
      "train loss:2.3251938034414126\n",
      "train loss:2.2982111776199385\n",
      "=== epoch:2, train acc:0.09333333333333334, test acc:0.0822 ===\n",
      "train loss:2.2913667116346472\n",
      "train loss:2.299497146992126\n",
      "train loss:2.319901185790573\n",
      "=== epoch:3, train acc:0.11, test acc:0.0863 ===\n",
      "train loss:2.2907243191322544\n",
      "train loss:2.2940115642020693\n",
      "train loss:2.2954562264933513\n",
      "=== epoch:4, train acc:0.11333333333333333, test acc:0.0895 ===\n",
      "train loss:2.287769471470877\n",
      "train loss:2.2869389695331286\n",
      "train loss:2.2873296764066713\n",
      "=== epoch:5, train acc:0.12333333333333334, test acc:0.0913 ===\n",
      "train loss:2.2871043631401595\n",
      "train loss:2.309980734196905\n",
      "train loss:2.294366043348253\n",
      "=== epoch:6, train acc:0.12666666666666668, test acc:0.0954 ===\n",
      "train loss:2.2845722842564653\n",
      "train loss:2.2844720410644372\n",
      "train loss:2.27630173642468\n",
      "=== epoch:7, train acc:0.13666666666666666, test acc:0.0969 ===\n",
      "train loss:2.30774540217209\n",
      "train loss:2.303459050229244\n",
      "train loss:2.293025940794659\n",
      "=== epoch:8, train acc:0.13333333333333333, test acc:0.1003 ===\n",
      "train loss:2.2820941230835605\n",
      "train loss:2.2933427662720534\n",
      "train loss:2.284182088727993\n",
      "=== epoch:9, train acc:0.13333333333333333, test acc:0.1032 ===\n",
      "train loss:2.2978590529269685\n",
      "train loss:2.281943178155265\n",
      "train loss:2.281962407704804\n",
      "=== epoch:10, train acc:0.14333333333333334, test acc:0.1112 ===\n",
      "train loss:2.293383040271948\n",
      "train loss:2.2895334133002248\n",
      "train loss:2.289319816823752\n",
      "=== epoch:11, train acc:0.15333333333333332, test acc:0.1159 ===\n",
      "train loss:2.2993104146990877\n",
      "train loss:2.2732685233088703\n",
      "train loss:2.270631500196792\n",
      "=== epoch:12, train acc:0.15333333333333332, test acc:0.1206 ===\n",
      "train loss:2.2775213650817454\n",
      "train loss:2.282771517659859\n",
      "train loss:2.272015592094043\n",
      "=== epoch:13, train acc:0.17, test acc:0.1261 ===\n",
      "train loss:2.27213948254312\n",
      "train loss:2.2624537031336387\n",
      "train loss:2.2689345074020513\n",
      "=== epoch:14, train acc:0.16666666666666666, test acc:0.1308 ===\n",
      "train loss:2.2762901866757534\n",
      "train loss:2.272750249714451\n",
      "train loss:2.2847482591181345\n",
      "=== epoch:15, train acc:0.18333333333333332, test acc:0.1425 ===\n",
      "train loss:2.264685169420344\n",
      "train loss:2.2881868258619393\n",
      "train loss:2.2857945999987983\n",
      "=== epoch:16, train acc:0.18666666666666668, test acc:0.1503 ===\n",
      "train loss:2.2731488831187296\n",
      "train loss:2.277680525964405\n",
      "train loss:2.2634757034063067\n",
      "=== epoch:17, train acc:0.2, test acc:0.1558 ===\n",
      "train loss:2.275020182413753\n",
      "train loss:2.2798554400768403\n",
      "train loss:2.2694961544986163\n",
      "=== epoch:18, train acc:0.21333333333333335, test acc:0.1629 ===\n",
      "train loss:2.271619716747636\n",
      "train loss:2.2695619697797804\n",
      "train loss:2.2779667473170777\n",
      "=== epoch:19, train acc:0.21666666666666667, test acc:0.1689 ===\n",
      "train loss:2.2845616304607588\n",
      "train loss:2.2686116815860156\n",
      "train loss:2.2606990696256277\n",
      "=== epoch:20, train acc:0.22666666666666666, test acc:0.1736 ===\n",
      "train loss:2.2445299600474446\n",
      "train loss:2.2663656619636523\n",
      "train loss:2.2657024286337757\n",
      "=== epoch:21, train acc:0.24, test acc:0.1821 ===\n",
      "train loss:2.2511324349971145\n",
      "train loss:2.257988176481582\n",
      "train loss:2.2568651456247473\n",
      "=== epoch:22, train acc:0.25, test acc:0.1949 ===\n",
      "train loss:2.2602499353879835\n",
      "train loss:2.255447705937392\n",
      "train loss:2.2702697742268207\n",
      "=== epoch:23, train acc:0.24333333333333335, test acc:0.1982 ===\n",
      "train loss:2.2515642801736204\n",
      "train loss:2.2483683197885886\n",
      "train loss:2.2775253099207804\n",
      "=== epoch:24, train acc:0.2633333333333333, test acc:0.207 ===\n",
      "train loss:2.2585484847994945\n",
      "train loss:2.2490207531378648\n",
      "train loss:2.25269494218041\n",
      "=== epoch:25, train acc:0.2733333333333333, test acc:0.2144 ===\n",
      "train loss:2.2644587252668624\n",
      "train loss:2.265503894749727\n",
      "train loss:2.2445609663853796\n",
      "=== epoch:26, train acc:0.27666666666666667, test acc:0.2197 ===\n",
      "train loss:2.2647218024437574\n",
      "train loss:2.2382766041730324\n",
      "train loss:2.2463503447385564\n",
      "=== epoch:27, train acc:0.2866666666666667, test acc:0.2258 ===\n",
      "train loss:2.2411270199710875\n",
      "train loss:2.2433721426569244\n",
      "train loss:2.246949795952219\n",
      "=== epoch:28, train acc:0.29333333333333333, test acc:0.2264 ===\n",
      "train loss:2.2657064425388223\n",
      "train loss:2.250859187987737\n",
      "train loss:2.2464186779962834\n",
      "=== epoch:29, train acc:0.30666666666666664, test acc:0.2351 ===\n",
      "train loss:2.2466796356265424\n",
      "train loss:2.2390226841231358\n",
      "train loss:2.2488767946062684\n",
      "=== epoch:30, train acc:0.31333333333333335, test acc:0.2405 ===\n",
      "train loss:2.2300712944085097\n",
      "train loss:2.2404405958861924\n",
      "train loss:2.254201761596859\n",
      "=== epoch:31, train acc:0.30333333333333334, test acc:0.2476 ===\n",
      "train loss:2.253236515394612\n",
      "train loss:2.2595740496910848\n",
      "train loss:2.243043353870881\n",
      "=== epoch:32, train acc:0.31, test acc:0.254 ===\n",
      "train loss:2.243591966586109\n",
      "train loss:2.2494673804383516\n",
      "train loss:2.2434260990989934\n",
      "=== epoch:33, train acc:0.31, test acc:0.2579 ===\n",
      "train loss:2.2378889833996194\n",
      "train loss:2.2512176231973386\n",
      "train loss:2.2344803996189797\n",
      "=== epoch:34, train acc:0.31666666666666665, test acc:0.2636 ===\n",
      "train loss:2.242660069160436\n",
      "train loss:2.226143894967466\n",
      "train loss:2.2315965027608637\n",
      "=== epoch:35, train acc:0.32, test acc:0.2693 ===\n",
      "train loss:2.2473588742593176\n",
      "train loss:2.230016722836502\n",
      "train loss:2.2408093643625584\n",
      "=== epoch:36, train acc:0.3233333333333333, test acc:0.2707 ===\n",
      "train loss:2.2335541285078677\n",
      "train loss:2.2489051777342923\n",
      "train loss:2.2313060018339033\n",
      "=== epoch:37, train acc:0.34, test acc:0.2749 ===\n",
      "train loss:2.24171711948\n",
      "train loss:2.2332467061797425\n",
      "train loss:2.2427363073538684\n",
      "=== epoch:38, train acc:0.35, test acc:0.2777 ===\n",
      "train loss:2.2289083993068757\n",
      "train loss:2.2274169195053646\n",
      "train loss:2.237290184007307\n",
      "=== epoch:39, train acc:0.35333333333333333, test acc:0.2799 ===\n",
      "train loss:2.247682029487257\n",
      "train loss:2.2429152004672166\n",
      "train loss:2.2363664801068195\n",
      "=== epoch:40, train acc:0.35333333333333333, test acc:0.2845 ===\n",
      "train loss:2.233494113018389\n",
      "train loss:2.233024840258881\n",
      "train loss:2.2380288007705285\n",
      "=== epoch:41, train acc:0.3566666666666667, test acc:0.2835 ===\n",
      "train loss:2.2130132069654955\n",
      "train loss:2.214402982895562\n",
      "train loss:2.21798657772888\n",
      "=== epoch:42, train acc:0.36333333333333334, test acc:0.2873 ===\n",
      "train loss:2.237109602655296\n",
      "train loss:2.2228713708976873\n",
      "train loss:2.2405125545097233\n",
      "=== epoch:43, train acc:0.36, test acc:0.2859 ===\n",
      "train loss:2.2136881159219874\n",
      "train loss:2.229901120703287\n",
      "train loss:2.232665149446718\n",
      "=== epoch:44, train acc:0.37333333333333335, test acc:0.2896 ===\n",
      "train loss:2.2187747767167334\n",
      "train loss:2.2247173495922112\n",
      "train loss:2.200843017807996\n",
      "=== epoch:45, train acc:0.38, test acc:0.2918 ===\n",
      "train loss:2.2218157029049705\n",
      "train loss:2.2089149166184523\n",
      "train loss:2.203469047352444\n",
      "=== epoch:46, train acc:0.38, test acc:0.2919 ===\n",
      "train loss:2.201241712742643\n",
      "train loss:2.2235833545498727\n",
      "train loss:2.215781882216668\n",
      "=== epoch:47, train acc:0.38333333333333336, test acc:0.2912 ===\n",
      "train loss:2.2055733078648347\n",
      "train loss:2.215325190830524\n",
      "train loss:2.2307243107253756\n",
      "=== epoch:48, train acc:0.38, test acc:0.2944 ===\n",
      "train loss:2.207297958892069\n",
      "train loss:2.213571393718179\n",
      "train loss:2.2064456385689057\n",
      "=== epoch:49, train acc:0.38666666666666666, test acc:0.2975 ===\n",
      "train loss:2.1929291866391525\n",
      "train loss:2.2321679537973673\n",
      "train loss:2.2054223280793157\n",
      "=== epoch:50, train acc:0.38333333333333336, test acc:0.2988 ===\n",
      "train loss:2.217274741966889\n",
      "train loss:2.2110753048425846\n",
      "train loss:2.2041033835896484\n",
      "=== epoch:51, train acc:0.38, test acc:0.3006 ===\n",
      "train loss:2.2154614585386785\n",
      "train loss:2.2048791254096862\n",
      "train loss:2.200632221383737\n",
      "=== epoch:52, train acc:0.38, test acc:0.3028 ===\n",
      "train loss:2.2024950403112054\n",
      "train loss:2.2265860715765258\n",
      "train loss:2.1940032677107237\n",
      "=== epoch:53, train acc:0.37666666666666665, test acc:0.2999 ===\n",
      "train loss:2.21530519800072\n",
      "train loss:2.194555899470888\n",
      "train loss:2.174730878335646\n",
      "=== epoch:54, train acc:0.37666666666666665, test acc:0.2983 ===\n",
      "train loss:2.1964718030860326\n",
      "train loss:2.1930941820312544\n",
      "train loss:2.209020991022166\n",
      "=== epoch:55, train acc:0.38, test acc:0.3005 ===\n",
      "train loss:2.208624881554267\n",
      "train loss:2.167977103648914\n",
      "train loss:2.194972282707455\n",
      "=== epoch:56, train acc:0.38333333333333336, test acc:0.3021 ===\n",
      "train loss:2.1870284110305516\n",
      "train loss:2.2055865736482514\n",
      "train loss:2.218408332468979\n",
      "=== epoch:57, train acc:0.38666666666666666, test acc:0.3044 ===\n",
      "train loss:2.1862701452239945\n",
      "train loss:2.185601547188972\n",
      "train loss:2.192821038282352\n",
      "=== epoch:58, train acc:0.38666666666666666, test acc:0.305 ===\n",
      "train loss:2.1759092319635824\n",
      "train loss:2.1980720218110106\n",
      "train loss:2.167570111111427\n",
      "=== epoch:59, train acc:0.3933333333333333, test acc:0.3069 ===\n",
      "train loss:2.1623701798917723\n",
      "train loss:2.1581426818226914\n",
      "train loss:2.155448687618041\n",
      "=== epoch:60, train acc:0.38666666666666666, test acc:0.3072 ===\n",
      "train loss:2.205344324111272\n",
      "train loss:2.1786259168686875\n",
      "train loss:2.202989151612154\n",
      "=== epoch:61, train acc:0.39, test acc:0.3093 ===\n",
      "train loss:2.1590341537514686\n",
      "train loss:2.1737382821982822\n",
      "train loss:2.181234578847182\n",
      "=== epoch:62, train acc:0.39, test acc:0.3079 ===\n",
      "train loss:2.192420441790242\n",
      "train loss:2.1906481133340003\n",
      "train loss:2.1722596879506524\n",
      "=== epoch:63, train acc:0.39666666666666667, test acc:0.3111 ===\n",
      "train loss:2.143284652695804\n",
      "train loss:2.176649622717737\n",
      "train loss:2.1648246651986947\n",
      "=== epoch:64, train acc:0.39, test acc:0.3095 ===\n",
      "train loss:2.16662800076504\n",
      "train loss:2.1691275319469723\n",
      "train loss:2.1602779499531284\n",
      "=== epoch:65, train acc:0.39, test acc:0.3064 ===\n",
      "train loss:2.167789516115361\n",
      "train loss:2.1636037649919833\n",
      "train loss:2.179973465024675\n",
      "=== epoch:66, train acc:0.39, test acc:0.3063 ===\n",
      "train loss:2.1287596599160388\n",
      "train loss:2.166337002465565\n",
      "train loss:2.1752261719272132\n",
      "=== epoch:67, train acc:0.3933333333333333, test acc:0.3072 ===\n",
      "train loss:2.180103408205041\n",
      "train loss:2.1752070148586666\n",
      "train loss:2.1476181812435713\n",
      "=== epoch:68, train acc:0.39, test acc:0.3097 ===\n",
      "train loss:2.189778293778828\n",
      "train loss:2.160639869174303\n",
      "train loss:2.181258937189941\n",
      "=== epoch:69, train acc:0.39666666666666667, test acc:0.3121 ===\n",
      "train loss:2.1757814820306867\n",
      "train loss:2.1990203325100337\n",
      "train loss:2.1596625305156936\n",
      "=== epoch:70, train acc:0.4, test acc:0.3152 ===\n",
      "train loss:2.1680389987578264\n",
      "train loss:2.160544288575112\n",
      "train loss:2.1739918545374577\n",
      "=== epoch:71, train acc:0.39666666666666667, test acc:0.3149 ===\n",
      "train loss:2.1504927315830438\n",
      "train loss:2.191247196547632\n",
      "train loss:2.180028570080562\n",
      "=== epoch:72, train acc:0.39666666666666667, test acc:0.3156 ===\n",
      "train loss:2.1586938580748454\n",
      "train loss:2.1259460073614944\n",
      "train loss:2.14541985208752\n",
      "=== epoch:73, train acc:0.39666666666666667, test acc:0.3144 ===\n",
      "train loss:2.1618114635037307\n",
      "train loss:2.15295419557645\n",
      "train loss:2.1509230449044496\n",
      "=== epoch:74, train acc:0.39666666666666667, test acc:0.3164 ===\n",
      "train loss:2.1282083860963974\n",
      "train loss:2.145548087547653\n",
      "train loss:2.090854955344691\n",
      "=== epoch:75, train acc:0.39666666666666667, test acc:0.3126 ===\n",
      "train loss:2.1708087288347717\n",
      "train loss:2.1394728896646837\n",
      "train loss:2.176056946945687\n",
      "=== epoch:76, train acc:0.3933333333333333, test acc:0.3129 ===\n",
      "train loss:2.1602459312255755\n",
      "train loss:2.11877788877313\n",
      "train loss:2.1472829827215816\n",
      "=== epoch:77, train acc:0.39666666666666667, test acc:0.3122 ===\n",
      "train loss:2.094756213492603\n",
      "train loss:2.1432978433682193\n",
      "train loss:2.1580508281710133\n",
      "=== epoch:78, train acc:0.3933333333333333, test acc:0.3125 ===\n",
      "train loss:2.1502495328623303\n",
      "train loss:2.102748348122475\n",
      "train loss:2.154983407039866\n",
      "=== epoch:79, train acc:0.3933333333333333, test acc:0.3165 ===\n",
      "train loss:2.092475352912636\n",
      "train loss:2.1529271286601173\n",
      "train loss:2.1464154355697276\n",
      "=== epoch:80, train acc:0.39, test acc:0.3179 ===\n",
      "train loss:2.1010576602752353\n",
      "train loss:2.1212458670666288\n",
      "train loss:2.132092104132897\n",
      "=== epoch:81, train acc:0.39, test acc:0.3199 ===\n",
      "train loss:2.1617153094869552\n",
      "train loss:2.149062229457107\n",
      "train loss:2.100163395071964\n",
      "=== epoch:82, train acc:0.39666666666666667, test acc:0.3205 ===\n",
      "train loss:2.1416289833042628\n",
      "train loss:2.1050060873587153\n",
      "train loss:2.088044134934312\n",
      "=== epoch:83, train acc:0.4, test acc:0.3246 ===\n",
      "train loss:2.177341910880721\n",
      "train loss:2.1042878658581996\n",
      "train loss:2.106470691700095\n",
      "=== epoch:84, train acc:0.4, test acc:0.3261 ===\n",
      "train loss:2.1299254160196757\n",
      "train loss:2.063534593158842\n",
      "train loss:2.129546041196286\n",
      "=== epoch:85, train acc:0.4, test acc:0.3258 ===\n",
      "train loss:2.0946330141499705\n",
      "train loss:2.099314945953075\n",
      "train loss:2.098752193640851\n",
      "=== epoch:86, train acc:0.4, test acc:0.3266 ===\n",
      "train loss:2.0877190843451556\n",
      "train loss:2.080861602515643\n",
      "train loss:2.0766395083457088\n",
      "=== epoch:87, train acc:0.4033333333333333, test acc:0.3283 ===\n",
      "train loss:2.127750754160062\n",
      "train loss:2.063102056878913\n",
      "train loss:2.0312149635188055\n",
      "=== epoch:88, train acc:0.4166666666666667, test acc:0.3278 ===\n",
      "train loss:2.088081186388991\n",
      "train loss:2.097240355338208\n",
      "train loss:2.085800300025622\n",
      "=== epoch:89, train acc:0.4166666666666667, test acc:0.3268 ===\n",
      "train loss:2.0568873469239954\n",
      "train loss:2.0845347183782876\n",
      "train loss:2.0574037139766586\n",
      "=== epoch:90, train acc:0.4033333333333333, test acc:0.3239 ===\n",
      "train loss:2.0353711552924945\n",
      "train loss:2.0640992948570474\n",
      "train loss:2.091237241027635\n",
      "=== epoch:91, train acc:0.4166666666666667, test acc:0.3245 ===\n",
      "train loss:2.1059987449322777\n",
      "train loss:2.0833730056204263\n",
      "train loss:2.0550842415279913\n",
      "=== epoch:92, train acc:0.4166666666666667, test acc:0.3262 ===\n",
      "train loss:2.04554051093431\n",
      "train loss:2.0581195798369443\n",
      "train loss:2.1258875551659857\n",
      "=== epoch:93, train acc:0.4166666666666667, test acc:0.3256 ===\n",
      "train loss:2.0988972234091174\n",
      "train loss:2.063695398901508\n",
      "train loss:2.000515990476656\n",
      "=== epoch:94, train acc:0.41333333333333333, test acc:0.3241 ===\n",
      "train loss:2.089528471929539\n",
      "train loss:2.0715738382203326\n",
      "train loss:2.031597737029351\n",
      "=== epoch:95, train acc:0.41333333333333333, test acc:0.3265 ===\n",
      "train loss:1.9999394800099533\n",
      "train loss:2.0359216709342247\n",
      "train loss:2.0549178367731997\n",
      "=== epoch:96, train acc:0.41333333333333333, test acc:0.3258 ===\n",
      "train loss:2.1151689248661545\n",
      "train loss:2.100935964400923\n",
      "train loss:2.032172446515604\n",
      "=== epoch:97, train acc:0.41333333333333333, test acc:0.3261 ===\n",
      "train loss:2.0603101458199764\n",
      "train loss:2.058138936800388\n",
      "train loss:2.0279842800157115\n",
      "=== epoch:98, train acc:0.4166666666666667, test acc:0.3245 ===\n",
      "train loss:2.0834614972999397\n",
      "train loss:2.0707836500146986\n",
      "train loss:2.0495462646710307\n",
      "=== epoch:99, train acc:0.41333333333333333, test acc:0.3245 ===\n",
      "train loss:2.0930878338343915\n",
      "train loss:2.0547223703917643\n",
      "train loss:2.0477875051963377\n",
      "=== epoch:100, train acc:0.42333333333333334, test acc:0.3281 ===\n",
      "train loss:2.080167663298638\n",
      "train loss:2.020386764551127\n",
      "train loss:1.9727395541893902\n",
      "=== epoch:101, train acc:0.42, test acc:0.3266 ===\n",
      "train loss:2.0253138646950775\n",
      "train loss:2.078591943113086\n",
      "train loss:2.0720318137479934\n",
      "=== epoch:102, train acc:0.42, test acc:0.3294 ===\n",
      "train loss:2.017750031467118\n",
      "train loss:2.0471298568710914\n",
      "train loss:2.0492259747575448\n",
      "=== epoch:103, train acc:0.42333333333333334, test acc:0.3307 ===\n",
      "train loss:2.047102273735966\n",
      "train loss:2.029464061554073\n",
      "train loss:2.066112002688166\n",
      "=== epoch:104, train acc:0.42333333333333334, test acc:0.3324 ===\n",
      "train loss:2.033259335015255\n",
      "train loss:2.0156946687253394\n",
      "train loss:1.920638576319632\n",
      "=== epoch:105, train acc:0.42, test acc:0.3312 ===\n",
      "train loss:1.9996429981516244\n",
      "train loss:1.9958594585163707\n",
      "train loss:2.0517507479521724\n",
      "=== epoch:106, train acc:0.42, test acc:0.333 ===\n",
      "train loss:2.0556617364993515\n",
      "train loss:2.0141365940370672\n",
      "train loss:2.0725346790922803\n",
      "=== epoch:107, train acc:0.42333333333333334, test acc:0.3351 ===\n",
      "train loss:2.004477261994778\n",
      "train loss:2.0685591299357453\n",
      "train loss:2.0855321923060064\n",
      "=== epoch:108, train acc:0.43, test acc:0.3395 ===\n",
      "train loss:2.0259074908522807\n",
      "train loss:2.0198639241217435\n",
      "train loss:1.9891100130138026\n",
      "=== epoch:109, train acc:0.4266666666666667, test acc:0.341 ===\n",
      "train loss:2.003322377743813\n",
      "train loss:1.9406188475926345\n",
      "train loss:1.9883253963579162\n",
      "=== epoch:110, train acc:0.4266666666666667, test acc:0.3399 ===\n",
      "train loss:2.042385243836815\n",
      "train loss:2.0130267823944985\n",
      "train loss:1.9716879107662277\n",
      "=== epoch:111, train acc:0.4266666666666667, test acc:0.3413 ===\n",
      "train loss:1.9727358544503837\n",
      "train loss:2.0226082852397997\n",
      "train loss:1.960011003227431\n",
      "=== epoch:112, train acc:0.4266666666666667, test acc:0.3419 ===\n",
      "train loss:1.9829902014855494\n",
      "train loss:1.890245921196796\n",
      "train loss:1.9739773934347762\n",
      "=== epoch:113, train acc:0.4266666666666667, test acc:0.3421 ===\n",
      "train loss:1.9862261039375533\n",
      "train loss:2.001760494119281\n",
      "train loss:1.997114312330706\n",
      "=== epoch:114, train acc:0.4266666666666667, test acc:0.3432 ===\n",
      "train loss:2.0289508426482565\n",
      "train loss:2.0090278987521457\n",
      "train loss:1.9075885561596988\n",
      "=== epoch:115, train acc:0.4266666666666667, test acc:0.3453 ===\n",
      "train loss:1.8733594152321535\n",
      "train loss:1.9411248436666677\n",
      "train loss:1.9652609601424342\n",
      "=== epoch:116, train acc:0.4266666666666667, test acc:0.3454 ===\n",
      "train loss:1.978100953617689\n",
      "train loss:1.953489973583759\n",
      "train loss:1.9443022333407203\n",
      "=== epoch:117, train acc:0.4266666666666667, test acc:0.3439 ===\n",
      "train loss:1.9696597648355214\n",
      "train loss:1.875710586090201\n",
      "train loss:1.9598884907494036\n",
      "=== epoch:118, train acc:0.4266666666666667, test acc:0.3454 ===\n",
      "train loss:1.9827635698932877\n",
      "train loss:2.0086030704600017\n",
      "train loss:1.8651899019735483\n",
      "=== epoch:119, train acc:0.4266666666666667, test acc:0.3465 ===\n",
      "train loss:1.9051416458386106\n",
      "train loss:1.9849639073816043\n",
      "train loss:1.9279772563968822\n",
      "=== epoch:120, train acc:0.4266666666666667, test acc:0.3454 ===\n",
      "train loss:1.942858956117747\n",
      "train loss:1.8772970012249257\n",
      "train loss:1.9160527181750162\n",
      "=== epoch:121, train acc:0.4266666666666667, test acc:0.3492 ===\n",
      "train loss:1.9295367051649481\n",
      "train loss:1.9642521736797152\n",
      "train loss:1.944296646597486\n",
      "=== epoch:122, train acc:0.4266666666666667, test acc:0.3493 ===\n",
      "train loss:1.8542675747400548\n",
      "train loss:1.8940027651891962\n",
      "train loss:1.9087829344435823\n",
      "=== epoch:123, train acc:0.4266666666666667, test acc:0.3478 ===\n",
      "train loss:1.837853912024886\n",
      "train loss:1.835651163254002\n",
      "train loss:1.937297477155652\n",
      "=== epoch:124, train acc:0.4266666666666667, test acc:0.3483 ===\n",
      "train loss:1.9292854445057919\n",
      "train loss:1.949833960618949\n",
      "train loss:1.8031040416074293\n",
      "=== epoch:125, train acc:0.4266666666666667, test acc:0.3502 ===\n",
      "train loss:1.8794837973884966\n",
      "train loss:1.8755275708363024\n",
      "train loss:1.9319690852859481\n",
      "=== epoch:126, train acc:0.43, test acc:0.3506 ===\n",
      "train loss:1.8361378976771414\n",
      "train loss:1.9973098868410395\n",
      "train loss:1.8658203052099835\n",
      "=== epoch:127, train acc:0.43333333333333335, test acc:0.3509 ===\n",
      "train loss:1.9666741913404417\n",
      "train loss:1.9715633067002734\n",
      "train loss:1.8749180494149953\n",
      "=== epoch:128, train acc:0.44333333333333336, test acc:0.3566 ===\n",
      "train loss:1.7947797608994835\n",
      "train loss:1.7944588050346342\n",
      "train loss:1.886471227138574\n",
      "=== epoch:129, train acc:0.44, test acc:0.3537 ===\n",
      "train loss:1.8186917536473957\n",
      "train loss:1.8885488769742227\n",
      "train loss:1.832927608920508\n",
      "=== epoch:130, train acc:0.44, test acc:0.354 ===\n",
      "train loss:1.8030760456632007\n",
      "train loss:1.8551555380520757\n",
      "train loss:1.8837751291668479\n",
      "=== epoch:131, train acc:0.44, test acc:0.3537 ===\n",
      "train loss:1.8736423718140005\n",
      "train loss:1.8566688401963702\n",
      "train loss:1.8897236018535046\n",
      "=== epoch:132, train acc:0.44, test acc:0.3569 ===\n",
      "train loss:1.856210477798829\n",
      "train loss:1.7659338852799311\n",
      "train loss:1.9030632998796568\n",
      "=== epoch:133, train acc:0.44333333333333336, test acc:0.3591 ===\n",
      "train loss:1.904264869403946\n",
      "train loss:1.9229315676394\n",
      "train loss:1.7687550375310508\n",
      "=== epoch:134, train acc:0.44333333333333336, test acc:0.3599 ===\n",
      "train loss:1.7947796920986858\n",
      "train loss:1.795115275896558\n",
      "train loss:1.8285832753716553\n",
      "=== epoch:135, train acc:0.44, test acc:0.3627 ===\n",
      "train loss:1.9171658937097111\n",
      "train loss:1.8745563253082609\n",
      "train loss:1.797477052441916\n",
      "=== epoch:136, train acc:0.44666666666666666, test acc:0.3686 ===\n",
      "train loss:1.888731563948726\n",
      "train loss:1.8877386509960519\n",
      "train loss:1.7917867898531739\n",
      "=== epoch:137, train acc:0.44666666666666666, test acc:0.3682 ===\n",
      "train loss:1.9065584471433972\n",
      "train loss:1.8449233316979183\n",
      "train loss:1.8388465499436553\n",
      "=== epoch:138, train acc:0.45, test acc:0.3724 ===\n",
      "train loss:1.8870474974898233\n",
      "train loss:1.8474891906155437\n",
      "train loss:1.8039063267117887\n",
      "=== epoch:139, train acc:0.45, test acc:0.3737 ===\n",
      "train loss:1.913580934709877\n",
      "train loss:1.740103904308065\n",
      "train loss:1.7609208574654958\n",
      "=== epoch:140, train acc:0.45, test acc:0.3773 ===\n",
      "train loss:1.889238923565594\n",
      "train loss:1.6949050487577526\n",
      "train loss:1.8064433655583603\n",
      "=== epoch:141, train acc:0.4533333333333333, test acc:0.3806 ===\n",
      "train loss:1.748273410507483\n",
      "train loss:1.7897705373643493\n",
      "train loss:1.6678937732138381\n",
      "=== epoch:142, train acc:0.4533333333333333, test acc:0.3793 ===\n",
      "train loss:1.7770976093352906\n",
      "train loss:1.7697564956432685\n",
      "train loss:1.7734462504581165\n",
      "=== epoch:143, train acc:0.4533333333333333, test acc:0.3791 ===\n",
      "train loss:1.7169685939634831\n",
      "train loss:1.7629998543709047\n",
      "train loss:1.7051992719068332\n",
      "=== epoch:144, train acc:0.4533333333333333, test acc:0.3813 ===\n",
      "train loss:1.9178739855680227\n",
      "train loss:1.828846482568438\n",
      "train loss:1.751883569789391\n",
      "=== epoch:145, train acc:0.4633333333333333, test acc:0.3839 ===\n",
      "train loss:1.8460814837480901\n",
      "train loss:1.7887887205290154\n",
      "train loss:1.798523877698865\n",
      "=== epoch:146, train acc:0.46, test acc:0.3809 ===\n",
      "train loss:1.7886633991918885\n",
      "train loss:1.7197089321719081\n",
      "train loss:1.7953804869321646\n",
      "=== epoch:147, train acc:0.46, test acc:0.3844 ===\n",
      "train loss:1.8054937981367205\n",
      "train loss:1.7285967171722094\n",
      "train loss:1.7245224953203098\n",
      "=== epoch:148, train acc:0.4666666666666667, test acc:0.3856 ===\n",
      "train loss:1.8440870295011969\n",
      "train loss:1.7873985587290122\n",
      "train loss:1.5924180660519378\n",
      "=== epoch:149, train acc:0.45666666666666667, test acc:0.386 ===\n",
      "train loss:1.7477225604141458\n",
      "train loss:1.727560812192461\n",
      "train loss:1.7000703505221815\n",
      "=== epoch:150, train acc:0.4633333333333333, test acc:0.383 ===\n",
      "train loss:1.7574267913595938\n",
      "train loss:1.7568230814588406\n",
      "train loss:1.7068368021194344\n",
      "=== epoch:151, train acc:0.46, test acc:0.3854 ===\n",
      "train loss:1.7215152828751914\n",
      "train loss:1.7525836718516004\n",
      "train loss:1.7811759249978212\n",
      "=== epoch:152, train acc:0.46, test acc:0.3878 ===\n",
      "train loss:1.7472661037998778\n",
      "train loss:1.730738490757807\n",
      "train loss:1.696859663537364\n",
      "=== epoch:153, train acc:0.46, test acc:0.392 ===\n",
      "train loss:1.6100839651518957\n",
      "train loss:1.6726192901407702\n",
      "train loss:1.7122773903414998\n",
      "=== epoch:154, train acc:0.4666666666666667, test acc:0.3931 ===\n",
      "train loss:1.615922578472493\n",
      "train loss:1.6645146871327798\n",
      "train loss:1.6576286514276837\n",
      "=== epoch:155, train acc:0.46, test acc:0.3906 ===\n",
      "train loss:1.7252752647314709\n",
      "train loss:1.6736173355260309\n",
      "train loss:1.6664539459956103\n",
      "=== epoch:156, train acc:0.46, test acc:0.3908 ===\n",
      "train loss:1.704434234650588\n",
      "train loss:1.75360382958126\n",
      "train loss:1.766856752681054\n",
      "=== epoch:157, train acc:0.46, test acc:0.3927 ===\n",
      "train loss:1.6824997655522755\n",
      "train loss:1.793909399469719\n",
      "train loss:1.6557591700777663\n",
      "=== epoch:158, train acc:0.46, test acc:0.3945 ===\n",
      "train loss:1.604908212242097\n",
      "train loss:1.7539721838818378\n",
      "train loss:1.704809190940923\n",
      "=== epoch:159, train acc:0.4633333333333333, test acc:0.398 ===\n",
      "train loss:1.754904673646652\n",
      "train loss:1.5571754888997211\n",
      "train loss:1.6916879362002641\n",
      "=== epoch:160, train acc:0.4633333333333333, test acc:0.4028 ===\n",
      "train loss:1.7754174751116099\n",
      "train loss:1.6476049296035598\n",
      "train loss:1.6996460742965054\n",
      "=== epoch:161, train acc:0.47, test acc:0.4048 ===\n",
      "train loss:1.7308549385100573\n",
      "train loss:1.6845080775438515\n",
      "train loss:1.6237441868773415\n",
      "=== epoch:162, train acc:0.47, test acc:0.4041 ===\n",
      "train loss:1.6755574967575988\n",
      "train loss:1.6797016661625417\n",
      "train loss:1.7225646478655359\n",
      "=== epoch:163, train acc:0.47333333333333333, test acc:0.4057 ===\n",
      "train loss:1.6622958857669996\n",
      "train loss:1.6151952735766886\n",
      "train loss:1.585125592154868\n",
      "=== epoch:164, train acc:0.4866666666666667, test acc:0.4086 ===\n",
      "train loss:1.675001063460416\n",
      "train loss:1.7090565808454972\n",
      "train loss:1.686088842883143\n",
      "=== epoch:165, train acc:0.4866666666666667, test acc:0.4091 ===\n",
      "train loss:1.6218327488505904\n",
      "train loss:1.6399376397976522\n",
      "train loss:1.6028606846406976\n",
      "=== epoch:166, train acc:0.4866666666666667, test acc:0.4086 ===\n",
      "train loss:1.6782017137416128\n",
      "train loss:1.4470685531162883\n",
      "train loss:1.5917310842848145\n",
      "=== epoch:167, train acc:0.48333333333333334, test acc:0.4093 ===\n",
      "train loss:1.562009743372697\n",
      "train loss:1.6126168485946402\n",
      "train loss:1.6039329729529102\n",
      "=== epoch:168, train acc:0.4866666666666667, test acc:0.4093 ===\n",
      "train loss:1.6372057462218725\n",
      "train loss:1.6415548611689061\n",
      "train loss:1.64658402605443\n",
      "=== epoch:169, train acc:0.49, test acc:0.4101 ===\n",
      "train loss:1.5660759322066564\n",
      "train loss:1.5701457508751553\n",
      "train loss:1.6388525976943555\n",
      "=== epoch:170, train acc:0.5, test acc:0.4117 ===\n",
      "train loss:1.5673120872632456\n",
      "train loss:1.6237526526276158\n",
      "train loss:1.546700389352942\n",
      "=== epoch:171, train acc:0.49333333333333335, test acc:0.4112 ===\n",
      "train loss:1.5154403092995086\n",
      "train loss:1.4621809904115848\n",
      "train loss:1.5779097005039677\n",
      "=== epoch:172, train acc:0.49333333333333335, test acc:0.4118 ===\n",
      "train loss:1.5663911691641326\n",
      "train loss:1.584615859142403\n",
      "train loss:1.6301000881051502\n",
      "=== epoch:173, train acc:0.49666666666666665, test acc:0.4181 ===\n",
      "train loss:1.6103421498967043\n",
      "train loss:1.603802225505042\n",
      "train loss:1.4987421102143685\n",
      "=== epoch:174, train acc:0.5, test acc:0.4195 ===\n",
      "train loss:1.4300963749709197\n",
      "train loss:1.5841752571478056\n",
      "train loss:1.5488506001592308\n",
      "=== epoch:175, train acc:0.49666666666666665, test acc:0.418 ===\n",
      "train loss:1.5876729970797936\n",
      "train loss:1.6823275863522924\n",
      "train loss:1.558568993171383\n",
      "=== epoch:176, train acc:0.5, test acc:0.4241 ===\n",
      "train loss:1.5327899540259353\n",
      "train loss:1.5612233147945613\n",
      "train loss:1.5274422239245635\n",
      "=== epoch:177, train acc:0.5066666666666667, test acc:0.4232 ===\n",
      "train loss:1.623657882687427\n",
      "train loss:1.5087868543298268\n",
      "train loss:1.5451164101084174\n",
      "=== epoch:178, train acc:0.5066666666666667, test acc:0.4247 ===\n",
      "train loss:1.5459163407305343\n",
      "train loss:1.4059780308843113\n",
      "train loss:1.599711231228854\n",
      "=== epoch:179, train acc:0.5133333333333333, test acc:0.428 ===\n",
      "train loss:1.6196929386682826\n",
      "train loss:1.4497773356642\n",
      "train loss:1.5598446554365468\n",
      "=== epoch:180, train acc:0.52, test acc:0.4312 ===\n",
      "train loss:1.5805924369764452\n",
      "train loss:1.4621081863426744\n",
      "train loss:1.5134285484591055\n",
      "=== epoch:181, train acc:0.53, test acc:0.43 ===\n",
      "train loss:1.5757269343369615\n",
      "train loss:1.4961096396628124\n",
      "train loss:1.5049074689015867\n",
      "=== epoch:182, train acc:0.53, test acc:0.4304 ===\n",
      "train loss:1.4198033955283893\n",
      "train loss:1.501131225703387\n",
      "train loss:1.435778384569713\n",
      "=== epoch:183, train acc:0.5166666666666667, test acc:0.4322 ===\n",
      "train loss:1.3724458718897636\n",
      "train loss:1.5252758135751343\n",
      "train loss:1.5786285072989346\n",
      "=== epoch:184, train acc:0.52, test acc:0.4339 ===\n",
      "train loss:1.5518170430928953\n",
      "train loss:1.5541070850807548\n",
      "train loss:1.5269054838988794\n",
      "=== epoch:185, train acc:0.5566666666666666, test acc:0.4447 ===\n",
      "train loss:1.4845469763968657\n",
      "train loss:1.4626921508619788\n",
      "train loss:1.44848901698145\n",
      "=== epoch:186, train acc:0.5366666666666666, test acc:0.4428 ===\n",
      "train loss:1.454428391308527\n",
      "train loss:1.4293776510898746\n",
      "train loss:1.4243388049986168\n",
      "=== epoch:187, train acc:0.5366666666666666, test acc:0.4429 ===\n",
      "train loss:1.353112170606878\n",
      "train loss:1.5487930442286797\n",
      "train loss:1.3725174505085127\n",
      "=== epoch:188, train acc:0.5266666666666666, test acc:0.4421 ===\n",
      "train loss:1.3105924262962487\n",
      "train loss:1.4866339210915651\n",
      "train loss:1.539475270883237\n",
      "=== epoch:189, train acc:0.5333333333333333, test acc:0.4436 ===\n",
      "train loss:1.528874251932504\n",
      "train loss:1.4659804330919493\n",
      "train loss:1.3483219792730055\n",
      "=== epoch:190, train acc:0.5433333333333333, test acc:0.4492 ===\n",
      "train loss:1.5895140085738435\n",
      "train loss:1.461502661141968\n",
      "train loss:1.3967871864194856\n",
      "=== epoch:191, train acc:0.5533333333333333, test acc:0.4547 ===\n",
      "train loss:1.332646542481888\n",
      "train loss:1.3671259010978989\n",
      "train loss:1.427785973976504\n",
      "=== epoch:192, train acc:0.5633333333333334, test acc:0.4586 ===\n",
      "train loss:1.4887354241520077\n",
      "train loss:1.3216550931947908\n",
      "train loss:1.328341355465204\n",
      "=== epoch:193, train acc:0.5566666666666666, test acc:0.4574 ===\n",
      "train loss:1.3845767060664331\n",
      "train loss:1.3740434749517203\n",
      "train loss:1.4395305113776675\n",
      "=== epoch:194, train acc:0.5566666666666666, test acc:0.4588 ===\n",
      "train loss:1.4080846432989742\n",
      "train loss:1.3706964446679817\n",
      "train loss:1.2763218414742175\n",
      "=== epoch:195, train acc:0.5466666666666666, test acc:0.4552 ===\n",
      "train loss:1.5719783499555497\n",
      "train loss:1.454913936920759\n",
      "train loss:1.5915179062515359\n",
      "=== epoch:196, train acc:0.55, test acc:0.4607 ===\n",
      "train loss:1.4039766902462245\n",
      "train loss:1.4296014736919387\n",
      "train loss:1.4149607743934096\n",
      "=== epoch:197, train acc:0.5633333333333334, test acc:0.4637 ===\n",
      "train loss:1.3650739280968447\n",
      "train loss:1.5371414490963111\n",
      "train loss:1.3758094477780616\n",
      "=== epoch:198, train acc:0.5633333333333334, test acc:0.4686 ===\n",
      "train loss:1.4428958018426297\n",
      "train loss:1.292437879273496\n",
      "train loss:1.3798236288785666\n",
      "=== epoch:199, train acc:0.5666666666666667, test acc:0.4733 ===\n",
      "train loss:1.279325861698258\n",
      "train loss:1.3296902400852395\n",
      "train loss:1.4468093050218738\n",
      "=== epoch:200, train acc:0.57, test acc:0.4696 ===\n",
      "train loss:1.4070578430584524\n",
      "train loss:1.3255046481850559\n",
      "train loss:1.417976615074818\n",
      "=== epoch:201, train acc:0.5733333333333334, test acc:0.4698 ===\n",
      "train loss:1.2895640172864256\n",
      "train loss:1.2621662302449976\n",
      "train loss:1.3882968179128952\n",
      "=== epoch:202, train acc:0.57, test acc:0.4747 ===\n",
      "train loss:1.3598728127464725\n",
      "train loss:1.3397077588342439\n",
      "train loss:1.316614645184317\n",
      "=== epoch:203, train acc:0.58, test acc:0.4762 ===\n",
      "train loss:1.35157749031001\n",
      "train loss:1.286724374632441\n",
      "train loss:1.4252660572548863\n",
      "=== epoch:204, train acc:0.59, test acc:0.4808 ===\n",
      "train loss:1.3837331360113279\n",
      "train loss:1.404162321417771\n",
      "train loss:1.2480237391630102\n",
      "=== epoch:205, train acc:0.5966666666666667, test acc:0.4844 ===\n",
      "train loss:1.4064275984541885\n",
      "train loss:1.356006255337374\n",
      "train loss:1.3139104233476688\n",
      "=== epoch:206, train acc:0.61, test acc:0.4898 ===\n",
      "train loss:1.363111479825801\n",
      "train loss:1.2802667386706725\n",
      "train loss:1.3364462449702625\n",
      "=== epoch:207, train acc:0.6133333333333333, test acc:0.4882 ===\n",
      "train loss:1.177830050127997\n",
      "train loss:1.1665510744178222\n",
      "train loss:1.2176880421605312\n",
      "=== epoch:208, train acc:0.6066666666666667, test acc:0.4888 ===\n",
      "train loss:1.2811456168988453\n",
      "train loss:1.3968973696955742\n",
      "train loss:1.2913788171282519\n",
      "=== epoch:209, train acc:0.6166666666666667, test acc:0.4884 ===\n",
      "train loss:1.2449705015104422\n",
      "train loss:1.1800158175513429\n",
      "train loss:1.3175747667638331\n",
      "=== epoch:210, train acc:0.6133333333333333, test acc:0.4869 ===\n",
      "train loss:1.256528673303457\n",
      "train loss:1.2702197515606422\n",
      "train loss:1.2579521061168768\n",
      "=== epoch:211, train acc:0.6066666666666667, test acc:0.4894 ===\n",
      "train loss:1.369291896182906\n",
      "train loss:1.2738534717579826\n",
      "train loss:1.2011997922270927\n",
      "=== epoch:212, train acc:0.6166666666666667, test acc:0.4953 ===\n",
      "train loss:1.3273299645442174\n",
      "train loss:1.0555202222886124\n",
      "train loss:1.3165689131141807\n",
      "=== epoch:213, train acc:0.6233333333333333, test acc:0.4998 ===\n",
      "train loss:1.331682976370182\n",
      "train loss:1.2187266484111887\n",
      "train loss:1.2541829679242225\n",
      "=== epoch:214, train acc:0.6266666666666667, test acc:0.5017 ===\n",
      "train loss:1.331427002732715\n",
      "train loss:1.3879203680546441\n",
      "train loss:1.2353957136047466\n",
      "=== epoch:215, train acc:0.6266666666666667, test acc:0.5083 ===\n",
      "train loss:1.172548408035595\n",
      "train loss:1.1383357405408012\n",
      "train loss:1.2363267280087122\n",
      "=== epoch:216, train acc:0.63, test acc:0.5044 ===\n",
      "train loss:1.2479694852355665\n",
      "train loss:1.2512495086926845\n",
      "train loss:1.2648995844572573\n",
      "=== epoch:217, train acc:0.6366666666666667, test acc:0.5114 ===\n",
      "train loss:1.3919415404662865\n",
      "train loss:1.270269846016457\n",
      "train loss:1.306642386924151\n",
      "=== epoch:218, train acc:0.64, test acc:0.5171 ===\n",
      "train loss:1.1496434185644993\n",
      "train loss:1.1629158821781196\n",
      "train loss:1.2059300334983285\n",
      "=== epoch:219, train acc:0.6433333333333333, test acc:0.5174 ===\n",
      "train loss:1.2309345232904882\n",
      "train loss:1.2060828204794776\n",
      "train loss:1.2977967739105614\n",
      "=== epoch:220, train acc:0.64, test acc:0.5158 ===\n",
      "train loss:1.1926703561833283\n",
      "train loss:1.175578011028115\n",
      "train loss:1.2625454618575858\n",
      "=== epoch:221, train acc:0.6433333333333333, test acc:0.5146 ===\n",
      "train loss:1.2286006675329957\n",
      "train loss:1.1989480993548183\n",
      "train loss:1.1861771463558597\n",
      "=== epoch:222, train acc:0.65, test acc:0.5182 ===\n",
      "train loss:1.267631724406539\n",
      "train loss:1.199132452209047\n",
      "train loss:1.264605384356693\n",
      "=== epoch:223, train acc:0.66, test acc:0.5269 ===\n",
      "train loss:1.1400570419541078\n",
      "train loss:1.1058868841714813\n",
      "train loss:1.0871678378051184\n",
      "=== epoch:224, train acc:0.6633333333333333, test acc:0.5269 ===\n",
      "train loss:1.0924448136052336\n",
      "train loss:1.2188227301059709\n",
      "train loss:1.1724115442212695\n",
      "=== epoch:225, train acc:0.6566666666666666, test acc:0.5233 ===\n",
      "train loss:1.1485239765336166\n",
      "train loss:1.1297063252595243\n",
      "train loss:1.2011969285832054\n",
      "=== epoch:226, train acc:0.67, test acc:0.5322 ===\n",
      "train loss:1.2889649093369453\n",
      "train loss:1.1800736530896025\n",
      "train loss:1.1078848989231105\n",
      "=== epoch:227, train acc:0.6833333333333333, test acc:0.5385 ===\n",
      "train loss:1.1776280028948758\n",
      "train loss:1.2019929064725665\n",
      "train loss:1.2896273437298271\n",
      "=== epoch:228, train acc:0.68, test acc:0.5368 ===\n",
      "train loss:1.10923816952234\n",
      "train loss:1.1841322433784314\n",
      "train loss:1.1022048600507866\n",
      "=== epoch:229, train acc:0.68, test acc:0.5395 ===\n",
      "train loss:1.1074112763796282\n",
      "train loss:1.142781182902366\n",
      "train loss:1.1938302741254223\n",
      "=== epoch:230, train acc:0.6833333333333333, test acc:0.5441 ===\n",
      "train loss:1.1323082032725202\n",
      "train loss:1.079812460676683\n",
      "train loss:1.1490353200690808\n",
      "=== epoch:231, train acc:0.68, test acc:0.5417 ===\n",
      "train loss:1.006116071492297\n",
      "train loss:1.1744356543297396\n",
      "train loss:1.1628720918215445\n",
      "=== epoch:232, train acc:0.68, test acc:0.5474 ===\n",
      "train loss:1.2061419306578067\n",
      "train loss:1.0805640470118636\n",
      "train loss:1.1285815009003846\n",
      "=== epoch:233, train acc:0.6866666666666666, test acc:0.5463 ===\n",
      "train loss:1.1183902600054048\n",
      "train loss:1.08287840116106\n",
      "train loss:1.0665555972633745\n",
      "=== epoch:234, train acc:0.68, test acc:0.5411 ===\n",
      "train loss:1.0247976057109294\n",
      "train loss:1.1528814385840402\n",
      "train loss:1.2035202145457762\n",
      "=== epoch:235, train acc:0.68, test acc:0.5417 ===\n",
      "train loss:1.2063777100964437\n",
      "train loss:0.9915602518064685\n",
      "train loss:1.0326872659000315\n",
      "=== epoch:236, train acc:0.6933333333333334, test acc:0.5428 ===\n",
      "train loss:1.1192379494712856\n",
      "train loss:1.078057393713337\n",
      "train loss:1.1383123133109785\n",
      "=== epoch:237, train acc:0.69, test acc:0.5493 ===\n",
      "train loss:1.2021293945524125\n",
      "train loss:1.1317694947561439\n",
      "train loss:0.9978529232460303\n",
      "=== epoch:238, train acc:0.68, test acc:0.5436 ===\n",
      "train loss:1.1194419164293685\n",
      "train loss:1.0396772677318347\n",
      "train loss:1.075086693813243\n",
      "=== epoch:239, train acc:0.69, test acc:0.5501 ===\n",
      "train loss:1.0688316573252141\n",
      "train loss:1.017492231398934\n",
      "train loss:1.0549925939694689\n",
      "=== epoch:240, train acc:0.7066666666666667, test acc:0.5549 ===\n",
      "train loss:1.0316828355072025\n",
      "train loss:1.2229500580014931\n",
      "train loss:1.0303804118198645\n",
      "=== epoch:241, train acc:0.71, test acc:0.5582 ===\n",
      "train loss:1.0815849813023442\n",
      "train loss:1.0348312487532383\n",
      "train loss:0.9896927241946056\n",
      "=== epoch:242, train acc:0.6966666666666667, test acc:0.56 ===\n",
      "train loss:1.0070099044470984\n",
      "train loss:0.9531618381324257\n",
      "train loss:0.9410151736600316\n",
      "=== epoch:243, train acc:0.7033333333333334, test acc:0.5588 ===\n",
      "train loss:0.9359221321031385\n",
      "train loss:1.068826075914241\n",
      "train loss:1.067028522799644\n",
      "=== epoch:244, train acc:0.7, test acc:0.5594 ===\n",
      "train loss:1.0865867692128217\n",
      "train loss:1.0257732093432634\n",
      "train loss:1.0252964433485046\n",
      "=== epoch:245, train acc:0.6933333333333334, test acc:0.553 ===\n",
      "train loss:1.017117300369367\n",
      "train loss:0.9673643390773222\n",
      "train loss:1.1580316169051463\n",
      "=== epoch:246, train acc:0.7, test acc:0.5529 ===\n",
      "train loss:1.0177303489582878\n",
      "train loss:1.0119362370639202\n",
      "train loss:1.0366601693124056\n",
      "=== epoch:247, train acc:0.6933333333333334, test acc:0.5553 ===\n",
      "train loss:1.0929764769342436\n",
      "train loss:1.0483117705218972\n",
      "train loss:0.9682895927403323\n",
      "=== epoch:248, train acc:0.6933333333333334, test acc:0.5586 ===\n",
      "train loss:1.038650151394768\n",
      "train loss:1.0040389824810712\n",
      "train loss:1.0251520588250653\n",
      "=== epoch:249, train acc:0.7033333333333334, test acc:0.5646 ===\n",
      "train loss:1.0922637637763275\n",
      "train loss:1.1055189414980808\n",
      "train loss:1.0112640712021586\n",
      "=== epoch:250, train acc:0.71, test acc:0.5672 ===\n",
      "train loss:0.9705330328498617\n",
      "train loss:0.8891833949953327\n",
      "train loss:0.9995575365287342\n",
      "=== epoch:251, train acc:0.7, test acc:0.5659 ===\n",
      "train loss:0.9512636769026455\n",
      "train loss:0.9539358680023966\n",
      "train loss:1.0270903042752644\n",
      "=== epoch:252, train acc:0.6933333333333334, test acc:0.5662 ===\n",
      "train loss:1.0301981473374597\n",
      "train loss:1.1620900677254142\n",
      "train loss:1.023380543074028\n",
      "=== epoch:253, train acc:0.7033333333333334, test acc:0.5674 ===\n",
      "train loss:0.9582699305542499\n",
      "train loss:1.046212870621599\n",
      "train loss:1.0775540795142076\n",
      "=== epoch:254, train acc:0.7233333333333334, test acc:0.5737 ===\n",
      "train loss:1.1256211018224094\n",
      "train loss:0.9830839539944202\n",
      "train loss:0.895936279841665\n",
      "=== epoch:255, train acc:0.7233333333333334, test acc:0.5787 ===\n",
      "train loss:1.07125580103197\n",
      "train loss:0.9859408271720731\n",
      "train loss:0.914993155252484\n",
      "=== epoch:256, train acc:0.7266666666666667, test acc:0.5834 ===\n",
      "train loss:0.879145717286685\n",
      "train loss:0.910596323970615\n",
      "train loss:0.9349006132238693\n",
      "=== epoch:257, train acc:0.7333333333333333, test acc:0.5839 ===\n",
      "train loss:0.9980942674729184\n",
      "train loss:0.9297399678611602\n",
      "train loss:0.9613412446699399\n",
      "=== epoch:258, train acc:0.73, test acc:0.5852 ===\n",
      "train loss:0.991416823659195\n",
      "train loss:0.980493288010056\n",
      "train loss:0.9077229361662189\n",
      "=== epoch:259, train acc:0.74, test acc:0.5872 ===\n",
      "train loss:0.8461188865216556\n",
      "train loss:0.9705624783900815\n",
      "train loss:0.8544970508590403\n",
      "=== epoch:260, train acc:0.74, test acc:0.5856 ===\n",
      "train loss:0.8973899042949904\n",
      "train loss:0.8793855152150095\n",
      "train loss:0.9736293023601202\n",
      "=== epoch:261, train acc:0.7333333333333333, test acc:0.5827 ===\n",
      "train loss:0.8092634811103587\n",
      "train loss:0.9139968570076856\n",
      "train loss:0.933989256048844\n",
      "=== epoch:262, train acc:0.73, test acc:0.5818 ===\n",
      "train loss:0.8319801941049226\n",
      "train loss:0.9389604162123972\n",
      "train loss:0.8770829009802255\n",
      "=== epoch:263, train acc:0.73, test acc:0.5819 ===\n",
      "train loss:0.8545438666077685\n",
      "train loss:0.8168105994032537\n",
      "train loss:0.8213138855776357\n",
      "=== epoch:264, train acc:0.7266666666666667, test acc:0.5826 ===\n",
      "train loss:1.0633244458638824\n",
      "train loss:0.9776251341309059\n",
      "train loss:0.8983406564982812\n",
      "=== epoch:265, train acc:0.7366666666666667, test acc:0.5818 ===\n",
      "train loss:0.8898739871859078\n",
      "train loss:0.901351407045822\n",
      "train loss:0.8791164300552192\n",
      "=== epoch:266, train acc:0.7333333333333333, test acc:0.5866 ===\n",
      "train loss:0.8538553781021403\n",
      "train loss:0.9561457928188769\n",
      "train loss:0.8737761764648204\n",
      "=== epoch:267, train acc:0.7366666666666667, test acc:0.5889 ===\n",
      "train loss:0.9210547485270985\n",
      "train loss:0.9123914822852001\n",
      "train loss:0.9143538273271146\n",
      "=== epoch:268, train acc:0.7433333333333333, test acc:0.5916 ===\n",
      "train loss:1.1274187307575934\n",
      "train loss:0.9710401054537207\n",
      "train loss:0.8621561388253189\n",
      "=== epoch:269, train acc:0.7566666666666667, test acc:0.5963 ===\n",
      "train loss:0.7314654798668079\n",
      "train loss:0.8156113311726172\n",
      "train loss:0.8812764598043227\n",
      "=== epoch:270, train acc:0.7466666666666667, test acc:0.5969 ===\n",
      "train loss:0.8721473054826331\n",
      "train loss:0.8591596680049998\n",
      "train loss:0.8810296011957\n",
      "=== epoch:271, train acc:0.7533333333333333, test acc:0.6014 ===\n",
      "train loss:0.838590363600086\n",
      "train loss:0.716729960135431\n",
      "train loss:0.7961180647666894\n",
      "=== epoch:272, train acc:0.7666666666666667, test acc:0.6042 ===\n",
      "train loss:0.8704676214703734\n",
      "train loss:0.8903221471016277\n",
      "train loss:0.7117240132563282\n",
      "=== epoch:273, train acc:0.78, test acc:0.6033 ===\n",
      "train loss:0.9755031276109051\n",
      "train loss:0.9754603668704432\n",
      "train loss:0.8295444251427929\n",
      "=== epoch:274, train acc:0.7733333333333333, test acc:0.6071 ===\n",
      "train loss:0.9259955210024657\n",
      "train loss:0.8383262695895662\n",
      "train loss:0.8177748914136086\n",
      "=== epoch:275, train acc:0.7866666666666666, test acc:0.6132 ===\n",
      "train loss:0.8739948276384384\n",
      "train loss:0.8141313384473186\n",
      "train loss:0.7866330683671903\n",
      "=== epoch:276, train acc:0.7933333333333333, test acc:0.6175 ===\n",
      "train loss:0.8816636212707762\n",
      "train loss:0.8276944024886101\n",
      "train loss:0.8336858441304759\n",
      "=== epoch:277, train acc:0.79, test acc:0.6162 ===\n",
      "train loss:0.8390810339776371\n",
      "train loss:0.9715938243700919\n",
      "train loss:0.8467185937146384\n",
      "=== epoch:278, train acc:0.7933333333333333, test acc:0.6226 ===\n",
      "train loss:0.7416797278800593\n",
      "train loss:0.721582543307719\n",
      "train loss:0.8657099071998599\n",
      "=== epoch:279, train acc:0.7866666666666666, test acc:0.6213 ===\n",
      "train loss:0.8282071987116082\n",
      "train loss:0.7648420760041259\n",
      "train loss:0.8933959526149253\n",
      "=== epoch:280, train acc:0.7933333333333333, test acc:0.6209 ===\n",
      "train loss:0.8434601627007013\n",
      "train loss:0.7833893516095009\n",
      "train loss:0.7340135213554885\n",
      "=== epoch:281, train acc:0.7866666666666666, test acc:0.6187 ===\n",
      "train loss:0.9486202876340702\n",
      "train loss:0.7423769927543379\n",
      "train loss:0.7961156916124635\n",
      "=== epoch:282, train acc:0.7966666666666666, test acc:0.6197 ===\n",
      "train loss:0.7077689862555667\n",
      "train loss:0.7627391530273571\n",
      "train loss:0.7236915558657379\n",
      "=== epoch:283, train acc:0.7866666666666666, test acc:0.6188 ===\n",
      "train loss:0.8087910878183512\n",
      "train loss:0.6704817976017469\n",
      "train loss:0.6757346164752492\n",
      "=== epoch:284, train acc:0.7966666666666666, test acc:0.6189 ===\n",
      "train loss:0.8117380993470636\n",
      "train loss:0.7367591420452014\n",
      "train loss:0.7512359804439428\n",
      "=== epoch:285, train acc:0.79, test acc:0.6218 ===\n",
      "train loss:0.7626264703521517\n",
      "train loss:0.7896539370010548\n",
      "train loss:0.7318416430654712\n",
      "=== epoch:286, train acc:0.7933333333333333, test acc:0.625 ===\n",
      "train loss:0.685549636382915\n",
      "train loss:0.728829672826636\n",
      "train loss:0.7485415076499174\n",
      "=== epoch:287, train acc:0.8033333333333333, test acc:0.6227 ===\n",
      "train loss:0.7026629163663783\n",
      "train loss:0.6745553839533904\n",
      "train loss:0.8524393877686102\n",
      "=== epoch:288, train acc:0.7966666666666666, test acc:0.6166 ===\n",
      "train loss:0.7126016701916751\n",
      "train loss:0.5973219395370197\n",
      "train loss:0.6735114582935303\n",
      "=== epoch:289, train acc:0.7966666666666666, test acc:0.6132 ===\n",
      "train loss:0.7214379710023792\n",
      "train loss:0.7671462920681822\n",
      "train loss:0.5920640159106106\n",
      "=== epoch:290, train acc:0.8, test acc:0.6182 ===\n",
      "train loss:0.7088436910607198\n",
      "train loss:0.7082677234752406\n",
      "train loss:0.8004640098125414\n",
      "=== epoch:291, train acc:0.7866666666666666, test acc:0.6135 ===\n",
      "train loss:0.6053845387784523\n",
      "train loss:0.6913597015686893\n",
      "train loss:0.7819527960926222\n",
      "=== epoch:292, train acc:0.7866666666666666, test acc:0.6154 ===\n",
      "train loss:0.6872126338047886\n",
      "train loss:0.8736717411217308\n",
      "train loss:0.7412424363805349\n",
      "=== epoch:293, train acc:0.7966666666666666, test acc:0.6176 ===\n",
      "train loss:0.6190383657488541\n",
      "train loss:0.7141978139002769\n",
      "train loss:0.6457762120430272\n",
      "=== epoch:294, train acc:0.8066666666666666, test acc:0.6175 ===\n",
      "train loss:0.7093261388547388\n",
      "train loss:0.639129994418167\n",
      "train loss:0.809224774403831\n",
      "=== epoch:295, train acc:0.8066666666666666, test acc:0.6202 ===\n",
      "train loss:0.6599341110641351\n",
      "train loss:0.7644991331530627\n",
      "train loss:0.7589872169422853\n",
      "=== epoch:296, train acc:0.81, test acc:0.6259 ===\n",
      "train loss:0.7988229343860505\n",
      "train loss:0.5865711999956382\n",
      "train loss:0.6909527974885858\n",
      "=== epoch:297, train acc:0.8066666666666666, test acc:0.6292 ===\n",
      "train loss:0.8824430256923432\n",
      "train loss:0.6697148029507818\n",
      "train loss:0.7034535337585229\n",
      "=== epoch:298, train acc:0.8233333333333334, test acc:0.6359 ===\n",
      "train loss:0.8073408990438243\n",
      "train loss:0.8617624075780292\n",
      "train loss:0.5703901017934887\n",
      "=== epoch:299, train acc:0.8133333333333334, test acc:0.637 ===\n",
      "train loss:0.5534655520174657\n",
      "train loss:0.6974336522584014\n",
      "train loss:0.8000428443933127\n",
      "=== epoch:300, train acc:0.8166666666666667, test acc:0.6391 ===\n",
      "train loss:0.7563178722069845\n",
      "train loss:0.7357650486857775\n",
      "train loss:0.7689834759898463\n",
      "=== epoch:301, train acc:0.8233333333333334, test acc:0.6406 ===\n",
      "train loss:0.8386410207829615\n",
      "train loss:0.5866637495404506\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.6433\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/Users/krc/Downloads/deep-learning-from-scratch-master')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net_extend import MultiLayerNetExtend\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임\n",
    "x_train = x_train[:300]\n",
    "t_train = t_train[:300]\n",
    "\n",
    "# 드롭아웃 사용 유무와 비울 설정 ========================\n",
    "use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False\n",
    "dropout_ratio = 0.2\n",
    "# ====================================================\n",
    "\n",
    "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                              output_size=10, use_dropout=use_dropout, dropout_ration=dropout_ratio)\n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=301, mini_batch_size=100,\n",
    "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\n",
    "trainer.train()\n",
    "\n",
    "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\n",
    "\n",
    "# 그래프 그리기==========\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\n",
    "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# 드롭아웃을 이용하면 표현력을 높이면서 오버피팅 억제 가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • 앙상블 학습 \n",
    "\n",
    ": 개별적으로 학습시킨 여러 모델의 출력을 평균 내어 추론하는 방식\n",
    "\n",
    "\n",
    "→ 학습 때 뉴런을 무작위로 삭제하는 행위를 매번 다른 모델 학습시키는 것으로 해석되는 드롭아웃과 앙상블은 밀접 \n",
    "\n",
    "\n",
    "#### • 하이퍼파라미터 \n",
    "\n",
    ": 예를 들어, 각 층의 뉴런 수, 배치 크기, 매개변수 갱신 시 학습률과 가중치 감소 \n",
    "\n",
    "→ 값을 적절히 설정하지 않으면 모델의 성능이 크게 떨어지기도 함\n",
    "\n",
    "##### ★ 주의할 점 ★\n",
    "\n",
    ": 하이퍼파라미터의 성능 평가 때는 시험 데이터를 사용해서 안 된다. \n",
    "\n",
    ": 시험 데이터를 사용해서 조정하면 하이퍼파라미터 값이 시험 데이터에 오버피팅되기 때문\n",
    "\n",
    "####\n",
    "• 검증 데이터 \n",
    "\n",
    ": 하이퍼파라미터 조정할 때 조정용 데이터 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def smooth_curve(x):\n",
    "    \"\"\"손실 함수의 그래프를 매끄럽게 하기 위해 사용\n",
    "    \n",
    "    참고：http://glowingpython.blogspot.jp/2012/02/convolution-with-numpy.html\n",
    "    \"\"\"\n",
    "    window_len = 11\n",
    "    s = np.r_[x[window_len-1:0:-1], x, x[-1:-window_len:-1]]\n",
    "    w = np.kaiser(window_len, 2)\n",
    "    y = np.convolve(w/w.sum(), s, mode='valid')\n",
    "    return y[5:len(y)-5]\n",
    "\n",
    "\n",
    "def shuffle_dataset(x, t):\n",
    "    \"\"\"데이터셋을 뒤섞는다.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : 훈련 데이터\n",
    "    t : 정답 레이블\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x, t : 뒤섞은 훈련 데이터와 정답 레이블\n",
    "    \"\"\"\n",
    "    permutation = np.random.permutation(x.shape[0])\n",
    "    x = x[permutation,:] if x.ndim == 2 else x[permutation,:,:,:]\n",
    "    t = t[permutation]\n",
    "\n",
    "    return x, t\n",
    "\n",
    "def conv_output_size(input_size, filter_size, stride=1, pad=0):\n",
    "    return (input_size + 2*pad - filter_size) / stride + 1\n",
    "\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
    "    return col\n",
    "\n",
    "\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"(im2col과 반대) 2차원 배열을 입력받아 다수의 이미지 묶음으로 변환한다.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col : 2차원 배열(입력 데이터)\n",
    "    input_shape : 원래 이미지 데이터의 형상（예：(10, 1, 28, 28)）\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    img : 변환된 이미지들\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_shape\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\n",
    "\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride*out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride*out_w\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\n",
    "\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • 하이퍼파라미터 핵심 \n",
    "\n",
    ": 하이퍼파라미터 '최적 값'이 존재하는 범위를 조금씩 줄여간다는 것\n",
    "\n",
    ": 범위 조금씩 줄이려면 대략적인 범위 설정, 무작위로 값 골라낸 후, 그 값으로 정확도 평가\n",
    "\n",
    ": 정확도 살피면서 이 작업 여러 번 반복하며 하이터파라미터의 '최적 값'의 범위를 좁혀가는 것\n",
    "\n",
    "\n",
    "\n",
    "#### • 로그 스케일 지정\n",
    "\n",
    ": 대략적으로 지정하는 것이 효과적 \n",
    "\n",
    ": 0.001에서 1,000 사이와 같이 '10의 거듭제곱' 단위로 범위 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • 하이퍼파라미터 최적화하는 방법\n",
    "\n",
    "0.  하이퍼파라미터 값 범위 설정\n",
    "\n",
    "1.  설정된 범위에서 하이퍼파라미터 값 무작위로 추출\n",
    "\n",
    "2.  1단계에서 샘플링한 값 사용해서 학습하고, 검증 데이터로 정확도 평가 ( 에폭은 작게 설정 )\n",
    "\n",
    "3.  1, 2단계 특정 횟수 반복하며, 결과 보고 하이퍼파라미터 범위 좁힘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val acc:0.09 | lr:7.655901072830214e-06, weight decay:1.2380689822465065e-05\n",
      "val acc:0.1 | lr:8.493437968211317e-05, weight decay:4.190597333078865e-06\n",
      "val acc:0.1 | lr:3.5726046879811797e-06, weight decay:8.613047530339327e-07\n",
      "val acc:0.14 | lr:5.888840100427415e-05, weight decay:4.912990425938222e-07\n",
      "val acc:0.08 | lr:5.44174450496251e-06, weight decay:3.7103995947882426e-07\n",
      "val acc:0.09 | lr:3.529498169871303e-06, weight decay:3.5853321079335914e-07\n",
      "val acc:0.13 | lr:0.0002746786553812494, weight decay:4.260663598185973e-07\n",
      "val acc:0.51 | lr:0.003597146907443921, weight decay:1.686630669806563e-05\n",
      "val acc:0.06 | lr:2.302799718444326e-05, weight decay:5.090350823032233e-05\n",
      "val acc:0.13 | lr:5.994272394001158e-06, weight decay:7.416807841578642e-07\n",
      "val acc:0.09 | lr:4.057481622828432e-05, weight decay:5.366491084919974e-07\n",
      "val acc:0.04 | lr:8.275374745288742e-05, weight decay:7.317969937685735e-08\n",
      "val acc:0.75 | lr:0.008658610169135677, weight decay:6.82701420346575e-05\n",
      "val acc:0.14 | lr:2.2980484809815964e-05, weight decay:2.1730049023758675e-06\n",
      "val acc:0.13 | lr:0.0007869295171848164, weight decay:8.323479511998547e-06\n",
      "val acc:0.03 | lr:6.633647964054766e-06, weight decay:7.761457375776612e-06\n",
      "val acc:0.12 | lr:4.0099420430604356e-05, weight decay:3.3306172699901616e-05\n",
      "val acc:0.13 | lr:0.00012879260300794357, weight decay:4.915363949567126e-07\n",
      "val acc:0.03 | lr:1.0537069638938376e-06, weight decay:4.5132903651251304e-08\n",
      "val acc:0.13 | lr:0.0001413710135480088, weight decay:1.3519526965689787e-08\n",
      "val acc:0.2 | lr:0.0010065687486053936, weight decay:4.6788259012090225e-05\n",
      "val acc:0.12 | lr:4.321340229770312e-06, weight decay:1.1406578277980702e-05\n",
      "val acc:0.11 | lr:0.0007734711415851343, weight decay:4.493347273375967e-06\n",
      "val acc:0.14 | lr:0.0005869811558164134, weight decay:2.3920218856482053e-06\n",
      "val acc:0.06 | lr:0.0010285347005039725, weight decay:2.8863331632050975e-06\n",
      "val acc:0.13 | lr:6.132440304809322e-05, weight decay:8.18371887226139e-07\n",
      "val acc:0.09 | lr:1.2072648979878256e-06, weight decay:8.026964808964272e-05\n",
      "val acc:0.09 | lr:2.318226912022402e-06, weight decay:1.0050668016741533e-05\n",
      "val acc:0.16 | lr:6.238458531239815e-06, weight decay:1.370894702559564e-05\n",
      "val acc:0.14 | lr:2.280349422390938e-06, weight decay:8.18475786787461e-05\n",
      "val acc:0.12 | lr:0.0002497317843186569, weight decay:5.4855969381571996e-08\n",
      "val acc:0.09 | lr:1.1223743733564035e-06, weight decay:2.039474005697421e-06\n",
      "val acc:0.1 | lr:1.361950231227184e-06, weight decay:1.2262115920853225e-07\n",
      "val acc:0.55 | lr:0.004381370998502676, weight decay:5.655494970369689e-07\n",
      "val acc:0.1 | lr:5.7819998889962135e-06, weight decay:3.4255252473956894e-08\n",
      "val acc:0.11 | lr:7.922225768347508e-06, weight decay:6.861854861405865e-05\n",
      "val acc:0.09 | lr:2.1668498417160466e-05, weight decay:1.685156116674866e-07\n",
      "val acc:0.14 | lr:0.00031078247924820276, weight decay:1.0274421611361463e-05\n",
      "val acc:0.36 | lr:0.002011974440496621, weight decay:7.079074737677169e-06\n",
      "val acc:0.12 | lr:1.6063602009757996e-06, weight decay:4.93653851963909e-07\n",
      "val acc:0.06 | lr:1.3759008244565846e-05, weight decay:7.657201726393443e-05\n",
      "val acc:0.08 | lr:2.3213835207429587e-05, weight decay:3.465090557876512e-05\n",
      "val acc:0.49 | lr:0.0035784050944031994, weight decay:5.097938469761369e-06\n",
      "val acc:0.1 | lr:9.351300154844087e-05, weight decay:7.985907238511125e-06\n",
      "val acc:0.1 | lr:1.4312257803809396e-05, weight decay:4.207488093390199e-08\n",
      "val acc:0.44 | lr:0.0025741362803793485, weight decay:9.53195939170495e-06\n",
      "val acc:0.1 | lr:8.483109172874351e-05, weight decay:2.666254609739044e-06\n",
      "val acc:0.06 | lr:2.5409631971759866e-05, weight decay:2.715748530633876e-06\n",
      "val acc:0.08 | lr:3.724043227669969e-06, weight decay:2.436043920984218e-06\n",
      "val acc:0.07 | lr:0.00017769914921182396, weight decay:2.8026761365408412e-06\n",
      "val acc:0.11 | lr:1.1395843766222263e-06, weight decay:4.0973184726345456e-05\n",
      "val acc:0.03 | lr:1.9187638490400777e-05, weight decay:1.289731258771521e-08\n",
      "val acc:0.07 | lr:3.00859869335981e-06, weight decay:1.5381170612152145e-07\n",
      "val acc:0.11 | lr:9.000904784495117e-06, weight decay:2.031354307914707e-05\n",
      "val acc:0.54 | lr:0.006007790607370047, weight decay:7.803510444688982e-07\n",
      "val acc:0.5 | lr:0.003545813401249771, weight decay:4.6489794187032634e-07\n",
      "val acc:0.34 | lr:0.0018297013064351897, weight decay:7.103679671397701e-05\n",
      "val acc:0.15 | lr:0.0001877157487817963, weight decay:5.54699274496856e-07\n",
      "val acc:0.57 | lr:0.006031437471726586, weight decay:7.639535938559495e-05\n",
      "val acc:0.11 | lr:1.1415278078084904e-06, weight decay:3.5536251481076447e-06\n",
      "val acc:0.04 | lr:4.930737816944336e-06, weight decay:1.046531149677827e-06\n",
      "val acc:0.11 | lr:2.8192524322173343e-06, weight decay:1.0534068625028682e-05\n",
      "val acc:0.22 | lr:0.0015742650032097188, weight decay:4.68143492206964e-08\n",
      "val acc:0.04 | lr:2.1831850589385616e-05, weight decay:4.8351180486628884e-08\n",
      "val acc:0.13 | lr:3.7050446038601886e-05, weight decay:8.860985782876363e-05\n",
      "val acc:0.1 | lr:0.00036205372877560766, weight decay:9.478598228551103e-06\n",
      "val acc:0.09 | lr:0.00014189875598931618, weight decay:1.9768747286394704e-06\n",
      "val acc:0.15 | lr:8.835753666799871e-06, weight decay:3.6148963052419104e-06\n",
      "val acc:0.66 | lr:0.004989781604047031, weight decay:1.4345942248653047e-07\n",
      "val acc:0.14 | lr:5.126427727718585e-06, weight decay:3.2628753572291793e-08\n",
      "val acc:0.14 | lr:1.960104908144754e-05, weight decay:2.846553414737328e-06\n",
      "val acc:0.07 | lr:3.0788670940035966e-06, weight decay:2.495742139705388e-07\n",
      "val acc:0.12 | lr:0.0003940746223249826, weight decay:7.078282162708108e-07\n",
      "val acc:0.17 | lr:0.0005934062703406432, weight decay:3.4572331349904333e-07\n",
      "val acc:0.14 | lr:5.2453142331329755e-06, weight decay:4.1838344053549697e-05\n",
      "val acc:0.21 | lr:0.0011411664954798803, weight decay:1.2089313315841345e-07\n",
      "val acc:0.06 | lr:1.9382993299550743e-06, weight decay:1.6994834174461352e-08\n",
      "val acc:0.07 | lr:4.578340762616352e-06, weight decay:1.756924665863039e-07\n",
      "val acc:0.07 | lr:8.064138140342816e-06, weight decay:1.8203450658738693e-08\n",
      "val acc:0.14 | lr:0.0003960956386337569, weight decay:1.4247115865067647e-07\n",
      "val acc:0.64 | lr:0.005812597309848938, weight decay:4.313018553425194e-07\n",
      "val acc:0.12 | lr:4.942192782972087e-06, weight decay:2.868877147798621e-06\n",
      "val acc:0.15 | lr:0.00025284161029883384, weight decay:3.568988554109758e-08\n",
      "val acc:0.41 | lr:0.003575270061095837, weight decay:2.290499410666092e-07\n",
      "val acc:0.34 | lr:0.0031398409576803878, weight decay:9.899633004972541e-05\n",
      "val acc:0.11 | lr:2.164658379720393e-05, weight decay:7.349317602473073e-06\n",
      "val acc:0.05 | lr:2.7666149924264308e-06, weight decay:2.9700492641210158e-05\n",
      "val acc:0.1 | lr:0.00020491164533837427, weight decay:7.906728113505707e-05\n",
      "val acc:0.76 | lr:0.005302908747705435, weight decay:5.59717108234422e-07\n",
      "val acc:0.49 | lr:0.003215034905243783, weight decay:1.3040221192443334e-08\n",
      "val acc:0.09 | lr:1.5643607374224183e-06, weight decay:9.203309645412033e-05\n",
      "val acc:0.13 | lr:0.0004738521395209406, weight decay:1.706219592836721e-08\n",
      "val acc:0.34 | lr:0.001630798192212476, weight decay:6.087172737254569e-06\n",
      "val acc:0.18 | lr:0.0007365194298705378, weight decay:7.401347870252189e-08\n",
      "val acc:0.06 | lr:2.883452745556659e-05, weight decay:8.8789309229545e-07\n",
      "val acc:0.31 | lr:0.001517740213951035, weight decay:4.644088282941939e-06\n",
      "val acc:0.08 | lr:6.593129260355825e-05, weight decay:3.6105005209493485e-08\n",
      "val acc:0.12 | lr:1.2551222297361934e-05, weight decay:1.57038345347609e-08\n",
      "val acc:0.67 | lr:0.007792459824929203, weight decay:2.063717988710639e-08\n",
      "val acc:0.12 | lr:1.6692274354306e-06, weight decay:2.3487626174259775e-08\n",
      "=========== Hyper-Parameter Optimization Result ===========\n",
      "Best-1(val acc:0.76) | lr:0.005302908747705435, weight decay:5.59717108234422e-07\n",
      "Best-2(val acc:0.75) | lr:0.008658610169135677, weight decay:6.82701420346575e-05\n",
      "Best-3(val acc:0.67) | lr:0.007792459824929203, weight decay:2.063717988710639e-08\n",
      "Best-4(val acc:0.66) | lr:0.004989781604047031, weight decay:1.4345942248653047e-07\n",
      "Best-5(val acc:0.64) | lr:0.005812597309848938, weight decay:4.313018553425194e-07\n",
      "Best-6(val acc:0.57) | lr:0.006031437471726586, weight decay:7.639535938559495e-05\n",
      "Best-7(val acc:0.55) | lr:0.004381370998502676, weight decay:5.655494970369689e-07\n",
      "Best-8(val acc:0.54) | lr:0.006007790607370047, weight decay:7.803510444688982e-07\n",
      "Best-9(val acc:0.51) | lr:0.003597146907443921, weight decay:1.686630669806563e-05\n",
      "Best-10(val acc:0.5) | lr:0.003545813401249771, weight decay:4.6489794187032634e-07\n",
      "Best-11(val acc:0.49) | lr:0.0035784050944031994, weight decay:5.097938469761369e-06\n",
      "Best-12(val acc:0.49) | lr:0.003215034905243783, weight decay:1.3040221192443334e-08\n",
      "Best-13(val acc:0.44) | lr:0.0025741362803793485, weight decay:9.53195939170495e-06\n",
      "Best-14(val acc:0.41) | lr:0.003575270061095837, weight decay:2.290499410666092e-07\n",
      "Best-15(val acc:0.36) | lr:0.002011974440496621, weight decay:7.079074737677169e-06\n",
      "Best-16(val acc:0.34) | lr:0.0018297013064351897, weight decay:7.103679671397701e-05\n",
      "Best-17(val acc:0.34) | lr:0.0031398409576803878, weight decay:9.899633004972541e-05\n",
      "Best-18(val acc:0.34) | lr:0.001630798192212476, weight decay:6.087172737254569e-06\n",
      "Best-19(val acc:0.31) | lr:0.001517740213951035, weight decay:4.644088282941939e-06\n",
      "Best-20(val acc:0.22) | lr:0.0015742650032097188, weight decay:4.68143492206964e-08\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 20 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 하이퍼파라미터 최적화\n",
    "\n",
    "import sys, os\n",
    "sys.path.append('/Users/krc/Downloads/deep-learning-from-scratch-master')  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from common.multi_layer_net import MultiLayerNet\n",
    "from common.util import shuffle_dataset\n",
    "from common.trainer import Trainer\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\n",
    "\n",
    "# 결과를 빠르게 얻기 위해 훈련 데이터를 줄임\n",
    "x_train = x_train[:500]\n",
    "t_train = t_train[:500]\n",
    "\n",
    "# 20%를 검증 데이터로 분할\n",
    "validation_rate = 0.20\n",
    "validation_num = int(x_train.shape[0] * validation_rate)\n",
    "x_train, t_train = shuffle_dataset(x_train, t_train)\n",
    "x_val = x_train[:validation_num]\n",
    "t_val = t_train[:validation_num]\n",
    "x_train = x_train[validation_num:]\n",
    "t_train = t_train[validation_num:]\n",
    "\n",
    "\n",
    "def __train(lr, weight_decay, epocs=50):\n",
    "    network = MultiLayerNet(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\n",
    "                            output_size=10, weight_decay_lambda=weight_decay)\n",
    "    trainer = Trainer(network, x_train, t_train, x_val, t_val,\n",
    "                      epochs=epocs, mini_batch_size=100,\n",
    "                      optimizer='sgd', optimizer_param={'lr': lr}, verbose=False)\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer.test_acc_list, trainer.train_acc_list\n",
    "\n",
    "\n",
    "# 하이퍼파라미터 무작위 탐색======================================\n",
    "optimization_trial = 100\n",
    "results_val = {}\n",
    "results_train = {}\n",
    "for _ in range(optimization_trial):\n",
    "    # 탐색한 하이퍼파라미터의 범위 지정===============\n",
    "    weight_decay = 10 ** np.random.uniform(-8, -4)\n",
    "    lr = 10 ** np.random.uniform(-6, -2)\n",
    "    # ================================================\n",
    "\n",
    "    val_acc_list, train_acc_list = __train(lr, weight_decay)\n",
    "    print(\"val acc:\" + str(val_acc_list[-1]) + \" | lr:\" + str(lr) + \", weight decay:\" + str(weight_decay))\n",
    "    key = \"lr:\" + str(lr) + \", weight decay:\" + str(weight_decay)\n",
    "    results_val[key] = val_acc_list\n",
    "    results_train[key] = train_acc_list\n",
    "\n",
    "# 그래프 그리기========================================================\n",
    "print(\"=========== Hyper-Parameter Optimization Result ===========\")\n",
    "graph_draw_num = 20\n",
    "col_num = 5\n",
    "row_num = int(np.ceil(graph_draw_num / col_num))\n",
    "i = 0\n",
    "\n",
    "for key, val_acc_list in sorted(results_val.items(), key=lambda x:x[1][-1], reverse=True):\n",
    "    print(\"Best-\" + str(i+1) + \"(val acc:\" + str(val_acc_list[-1]) + \") | \" + key)\n",
    "\n",
    "    plt.subplot(row_num, col_num, i+1)\n",
    "    plt.title(\"Best-\" + str(i+1))\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    if i % 5: plt.yticks([])\n",
    "    plt.xticks([])\n",
    "    x = np.arange(len(val_acc_list))\n",
    "    plt.plot(x, val_acc_list)\n",
    "    plt.plot(x, results_train[key], \"--\")\n",
    "    i += 1\n",
    "\n",
    "    if i >= graph_draw_num:\n",
    "        break\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 학습 잘 진행될 때 학습률은 0.001 ~0.01인 것을 알 수 있다. \n",
    "# 잘될 것 같은 값의 범위 관찰하고 범위 좁혀감 → 축소된 범위로 같은 작업 반복 \n",
    "# 적절한 값이 위치한 범위를 좁혀가다가 특정 단계에서 최종 하이퍼파라미터 값 선택\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6장 정리\n",
    "\n",
    "- 매개변수 갱신 방법 SGD, 모멘텀, AdaGrad, Adam 등 있다.\n",
    "\n",
    "- 가중치 초깃값 정하는 방법은 매우 중요\n",
    "\n",
    "- 가중치 초깃값으로는 'Xavier 초깃값'과 'He 초깃값'이 효과적이다.\n",
    "\n",
    "- 배치 정규화 이용하면 학습을 빠르게 진행할 수 있으며, 초깃값에 영향 덜 받게 된다.\n",
    "\n",
    "- 오버피팅 억제하는 정규화 기술에는 가중치 감소와 드롭아웃이 있다.\n",
    "\n",
    "- 하이퍼파라미터 값 탐색은 최적 값이 존재할 법한 범위를 좁히며 하는 것이 효과적이다. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4685cf8fad66a3b5e1d83fe36d59de227ad64c6532a47c7a0b6e8e6da20ed48b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('krc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
