{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 네트워크 \n",
    "\n",
    "\"Convolution - ReLU - Pooling - Affine - ReLU - Affine - Sofrmax\" 순으로 흐름\n",
    "\n",
    "\n",
    "##### • 초기화 때 받는 인수\n",
    "\n",
    "• input_dim : 입력 데이터(채널 수, 높이, 너비)의 차원    \n",
    "\n",
    "• conv_param : 합성곱 계층의 하이퍼파라미터(딕셔너리). 딕셔너리의 키는 다음과 같다.\n",
    "    \n",
    "     filter_num : 필터 수 \n",
    "    \n",
    "     filter_size : 필터의 크기\n",
    "    \n",
    "     stride : 스트라이드\n",
    "    \n",
    "     pad : 패딩\n",
    "\n",
    "•hidden_size : 은닉층(완전연결)의 뉴런 수 \n",
    "\n",
    "•output_size : 출력층(완전연결)의 뉴런 수 \n",
    "\n",
    "•weigth_init_std : 초기화 때의 가중치 표준편차"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"/Users/krc/Downloads/deep-learning-from-scratch-master\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from common.layers import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class SimpleConvNet:\n",
    "    \"\"\"단순한 합성곱 신경망\n",
    "    \n",
    "    conv - relu - pool - affine - relu - affine - softmax\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
    "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
    "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
    "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
    "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
    "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
    "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        input_size = input_dim[1]\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "\n",
    "        # 가중치 매개변수 초기화하는 부분\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                            np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                            np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * \\\n",
    "                            np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "\n",
    "        # 학습에 필요한 매개변수는 1번째 층의 합성곱 계층과 두 완전연결 계층의 가중치와 편향        \n",
    "\n",
    "        # CNN 구성하는 계층 생성\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                           conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "# 순서가 있는 딕셔너리인 layers에 계층들을 차례로 추가한다. \n",
    "# 마지막 softmaxwithloss는 last_layer라는 별도 변수에 저장 \n",
    "# 이상이 SimpleConvNet의 초기화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 초기화 마치고 추론 수행하는 predict 메서드와 손실 함수 값 구하는 loss 메서드 구현\n",
    "# 인수 x는 입력 데이터, t는 정답 레이블\n",
    "# 추론 수행하는 predict 메서드는 초기화 때 layers에 추가한 계층 맨 앞부터 차례로 forward 메서드 호출하며 다음 계층에 전달. \n",
    "# 손실 함수 구하는 loss 메서드는 predict 메서드 결과를 인수로 마지막 층의 forward 메서드를 호출한다. \n",
    "# 즉, 첫 계층부터 마지막 계층까지 forward를 처리한다.\n",
    "\n",
    "def loss(self, x, t):\n",
    "        \"\"\"손실 함수를 구한다.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "        \"\"\"\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차역전파법으로 기울기 구하기 \n",
    "# 매개변수의 기울기는 오차역전파법으로 구한다. 순전파와 역전파 반복\n",
    "\n",
    "def gradient(self, x, t):\n",
    "        \"\"\"기울기를 구한다(오차역전파법).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : 입력 데이터\n",
    "        t : 정답 레이블\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
    "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
    "            grads['b1']、grads['b2']、... 각 층의 편향\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "\n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "# grads라는 딕셔너리 변수에 각 가중치 매개변수의 기울기를 저장한다. \n",
    "# 이상이 SimpleConvNet 구현 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2996503053802866\n",
      "=== epoch:1, train acc:0.212, test acc:0.191 ===\n",
      "train loss:2.299052676840693\n",
      "train loss:2.29516774271261\n",
      "train loss:2.290626531417012\n",
      "train loss:2.2844686740519373\n",
      "train loss:2.277413789990815\n",
      "train loss:2.2604208803339243\n",
      "train loss:2.2480716341101874\n",
      "train loss:2.22728417192762\n",
      "train loss:2.218667540108635\n",
      "train loss:2.1781265850293066\n",
      "train loss:2.157468672433832\n",
      "train loss:2.1095974083067657\n",
      "train loss:2.0300300918619505\n",
      "train loss:2.0377314574444085\n",
      "train loss:1.9384866899674376\n",
      "train loss:1.9082044362899202\n",
      "train loss:1.8303643412988972\n",
      "train loss:1.779375702338008\n",
      "train loss:1.7143215749170266\n",
      "train loss:1.7147483913462285\n",
      "train loss:1.6220293164292312\n",
      "train loss:1.506146451632491\n",
      "train loss:1.4525620327974265\n",
      "train loss:1.2911792959391528\n",
      "train loss:1.2125542816003902\n",
      "train loss:1.200948965541546\n",
      "train loss:0.9730151650553113\n",
      "train loss:1.0746661860562716\n",
      "train loss:0.9868275550424876\n",
      "train loss:0.9769216087056464\n",
      "train loss:0.9368400340822863\n",
      "train loss:0.8863813535877947\n",
      "train loss:0.8040986033782943\n",
      "train loss:0.7234190363105921\n",
      "train loss:0.8253708512119349\n",
      "train loss:0.6663796075824857\n",
      "train loss:0.6982085252369146\n",
      "train loss:0.5713032914285676\n",
      "train loss:0.5908045578565114\n",
      "train loss:0.44109180213569277\n",
      "train loss:0.7925815293139554\n",
      "train loss:0.6129871354738468\n",
      "train loss:0.6484876866823724\n",
      "train loss:0.5502013423456809\n",
      "train loss:0.4706877831674328\n",
      "train loss:0.614210460079924\n",
      "train loss:0.7597925244072818\n",
      "train loss:0.6021811220738958\n",
      "train loss:0.7183773793577736\n",
      "train loss:0.5590524239361874\n",
      "=== epoch:2, train acc:0.811, test acc:0.786 ===\n",
      "train loss:0.32483709886367934\n",
      "train loss:0.6033368552944036\n",
      "train loss:0.5584147385595216\n",
      "train loss:0.444538268791604\n",
      "train loss:0.5926558514750455\n",
      "train loss:0.453616260028398\n",
      "train loss:0.5542179953246705\n",
      "train loss:0.4695401171262839\n",
      "train loss:0.501359342946933\n",
      "train loss:0.3896557654423492\n",
      "train loss:0.48496934131474206\n",
      "train loss:0.5607326176829196\n",
      "train loss:0.597594071297778\n",
      "train loss:0.5303633094564632\n",
      "train loss:0.3940725534553672\n",
      "train loss:0.453669964128532\n",
      "train loss:0.587548573322579\n",
      "train loss:0.4842136479269228\n",
      "train loss:0.4875964917982715\n",
      "train loss:0.3297626863477573\n",
      "train loss:0.5174575044464033\n",
      "train loss:0.46142965058680874\n",
      "train loss:0.41836301341010945\n",
      "train loss:0.43053877007618657\n",
      "train loss:0.3751533529383767\n",
      "train loss:0.3883260430111215\n",
      "train loss:0.3977071351492682\n",
      "train loss:0.43537849521972916\n",
      "train loss:0.42375554232840423\n",
      "train loss:0.47854265176029015\n",
      "train loss:0.6154562315916354\n",
      "train loss:0.26823624610621083\n",
      "train loss:0.345380291035607\n",
      "train loss:0.35674470864157803\n",
      "train loss:0.40252870063801455\n",
      "train loss:0.5903965989232879\n",
      "train loss:0.34666390869863484\n",
      "train loss:0.4225003292191755\n",
      "train loss:0.30111412819311406\n",
      "train loss:0.2768867657949554\n",
      "train loss:0.38280401603898456\n",
      "train loss:0.47938318176815786\n",
      "train loss:0.3567918685272284\n",
      "train loss:0.3732627600004794\n",
      "train loss:0.4855526934317583\n",
      "train loss:0.286406960469167\n",
      "train loss:0.2824932495349223\n",
      "train loss:0.487462958488616\n",
      "train loss:0.3302633822933334\n",
      "train loss:0.2322912808018695\n",
      "=== epoch:3, train acc:0.875, test acc:0.868 ===\n",
      "train loss:0.35351793052324154\n",
      "train loss:0.2943611571261473\n",
      "train loss:0.2937939094629131\n",
      "train loss:0.28437502556269717\n",
      "train loss:0.4780127799137792\n",
      "train loss:0.4048831640675045\n",
      "train loss:0.3349545081728315\n",
      "train loss:0.5173855160671689\n",
      "train loss:0.3531887706842671\n",
      "train loss:0.32610760593879545\n",
      "train loss:0.27939283321821173\n",
      "train loss:0.356933964371855\n",
      "train loss:0.3466295649631813\n",
      "train loss:0.42466467753356274\n",
      "train loss:0.3183435429125783\n",
      "train loss:0.3424552103677776\n",
      "train loss:0.2409682538145904\n",
      "train loss:0.3970208189851851\n",
      "train loss:0.3113485518855404\n",
      "train loss:0.2593970453001204\n",
      "train loss:0.4531650944400763\n",
      "train loss:0.2891514180460459\n",
      "train loss:0.26041243659495644\n",
      "train loss:0.3750671959844128\n",
      "train loss:0.2788543857692626\n",
      "train loss:0.3061981211191948\n",
      "train loss:0.2896923891357748\n",
      "train loss:0.29220050925315233\n",
      "train loss:0.307950870665971\n",
      "train loss:0.17497376095619313\n",
      "train loss:0.36186202753601016\n",
      "train loss:0.31354492454719035\n",
      "train loss:0.32916255954543966\n",
      "train loss:0.26538293359910137\n",
      "train loss:0.29537849212783757\n",
      "train loss:0.294472512384893\n",
      "train loss:0.27391675318264375\n",
      "train loss:0.22144912783330128\n",
      "train loss:0.282989748256934\n",
      "train loss:0.3356022999192082\n",
      "train loss:0.25428913951043514\n",
      "train loss:0.23257169640218564\n",
      "train loss:0.3213560994796467\n",
      "train loss:0.20651505617026408\n",
      "train loss:0.31111635032116897\n",
      "train loss:0.12256870791644346\n",
      "train loss:0.2574425608782221\n",
      "train loss:0.4043929135926551\n",
      "train loss:0.296985755818546\n",
      "train loss:0.412110552047488\n",
      "=== epoch:4, train acc:0.892, test acc:0.884 ===\n",
      "train loss:0.3322083112465365\n",
      "train loss:0.30204237301472314\n",
      "train loss:0.16301282793062036\n",
      "train loss:0.2684908915941714\n",
      "train loss:0.24084619314943878\n",
      "train loss:0.2324949772861547\n",
      "train loss:0.2290687567828357\n",
      "train loss:0.34707895068530314\n",
      "train loss:0.2518604311856745\n",
      "train loss:0.16800323338115652\n",
      "train loss:0.2535547314349202\n",
      "train loss:0.2037123204547292\n",
      "train loss:0.24558910527539318\n",
      "train loss:0.2588627235119548\n",
      "train loss:0.22790143907035587\n",
      "train loss:0.3918504739857687\n",
      "train loss:0.2720086228326006\n",
      "train loss:0.18387019448104408\n",
      "train loss:0.3025068254795468\n",
      "train loss:0.2232564896538856\n",
      "train loss:0.23226523188753195\n",
      "train loss:0.41910189954764254\n",
      "train loss:0.23486544674210655\n",
      "train loss:0.1503217962448867\n",
      "train loss:0.2598073243057027\n",
      "train loss:0.1896862643286705\n",
      "train loss:0.30250719136767346\n",
      "train loss:0.2541281940772921\n",
      "train loss:0.274446129741306\n",
      "train loss:0.2618636162108016\n",
      "train loss:0.37446789678404696\n",
      "train loss:0.27494547486270443\n",
      "train loss:0.3746351737688082\n",
      "train loss:0.21616149221117364\n",
      "train loss:0.2736358203559679\n",
      "train loss:0.19731656628075225\n",
      "train loss:0.49625428062788\n",
      "train loss:0.1697820568825482\n",
      "train loss:0.18283206505666066\n",
      "train loss:0.24237833636840897\n",
      "train loss:0.13784054607022703\n",
      "train loss:0.14225904457123456\n",
      "train loss:0.20701702329452248\n",
      "train loss:0.3288543699513191\n",
      "train loss:0.3221710658403214\n",
      "train loss:0.26675634095008566\n",
      "train loss:0.30845649713202755\n",
      "train loss:0.21050928551013004\n",
      "train loss:0.18547518407557542\n",
      "train loss:0.2280022959512643\n",
      "=== epoch:5, train acc:0.904, test acc:0.9 ===\n",
      "train loss:0.23070466927994418\n",
      "train loss:0.1652938699588297\n",
      "train loss:0.19823852820954002\n",
      "train loss:0.27089446068451206\n",
      "train loss:0.227654269622412\n",
      "train loss:0.20970886657022508\n",
      "train loss:0.4047732029412348\n",
      "train loss:0.30799388787585774\n",
      "train loss:0.3427489691731534\n",
      "train loss:0.2154214068674612\n",
      "train loss:0.3748376285647964\n",
      "train loss:0.21093440407578576\n",
      "train loss:0.22652385074458034\n",
      "train loss:0.17531558082017334\n",
      "train loss:0.45512700800041306\n",
      "train loss:0.23866322615845856\n",
      "train loss:0.3178509074131072\n",
      "train loss:0.2816681328309454\n",
      "train loss:0.31194888154171235\n",
      "train loss:0.16690625790329178\n",
      "train loss:0.2267387679819943\n",
      "train loss:0.14429791749711496\n",
      "train loss:0.2554991727974152\n",
      "train loss:0.31232355997163297\n",
      "train loss:0.318104952389367\n",
      "train loss:0.17307142370536102\n",
      "train loss:0.21316427434321963\n",
      "train loss:0.1754395768685561\n",
      "train loss:0.19856536835699248\n",
      "train loss:0.26097472646750963\n",
      "train loss:0.17185301869432737\n",
      "train loss:0.1870563539622354\n",
      "train loss:0.32801438526048526\n",
      "train loss:0.2707750826198435\n",
      "train loss:0.19022084918840104\n",
      "train loss:0.2181825853167311\n",
      "train loss:0.19421857634245068\n",
      "train loss:0.22009675576229204\n",
      "train loss:0.19825910655888893\n",
      "train loss:0.16906852326311048\n",
      "train loss:0.20769945339800228\n",
      "train loss:0.24009214405075024\n",
      "train loss:0.23366586387447763\n",
      "train loss:0.21482273414226324\n",
      "train loss:0.2228569051635448\n",
      "train loss:0.3150347494752486\n",
      "train loss:0.3062816426373363\n",
      "train loss:0.20590624915407763\n",
      "train loss:0.2304816660773033\n",
      "train loss:0.15125038457407056\n",
      "=== epoch:6, train acc:0.918, test acc:0.907 ===\n",
      "train loss:0.18071985527755963\n",
      "train loss:0.1735891720821392\n",
      "train loss:0.19759208220730745\n",
      "train loss:0.2320853417988503\n",
      "train loss:0.19409373998695403\n",
      "train loss:0.1918341627261003\n",
      "train loss:0.1741558677120032\n",
      "train loss:0.31556527666978273\n",
      "train loss:0.1323116118605068\n",
      "train loss:0.25223645929621147\n",
      "train loss:0.11275542244564825\n",
      "train loss:0.08729890264082064\n",
      "train loss:0.319174953382968\n",
      "train loss:0.12065991790247281\n",
      "train loss:0.11245711875046158\n",
      "train loss:0.1778125630200322\n",
      "train loss:0.19459861457564248\n",
      "train loss:0.2332950906618688\n",
      "train loss:0.2677257137650715\n",
      "train loss:0.2933377697746351\n",
      "train loss:0.10637292045506454\n",
      "train loss:0.2838833109644119\n",
      "train loss:0.1499925055243931\n",
      "train loss:0.13867396120744166\n",
      "train loss:0.25906062707070737\n",
      "train loss:0.11238697753926928\n",
      "train loss:0.23374203493075005\n",
      "train loss:0.1840083132966067\n",
      "train loss:0.16249740199767135\n",
      "train loss:0.16369498634733545\n",
      "train loss:0.15318305652619266\n",
      "train loss:0.17222134247761353\n",
      "train loss:0.18339746362930134\n",
      "train loss:0.17537618203554206\n",
      "train loss:0.07677227458366502\n",
      "train loss:0.25488283878736034\n",
      "train loss:0.13697378038020674\n",
      "train loss:0.3033284937772478\n",
      "train loss:0.11835628786657702\n",
      "train loss:0.16334618135980467\n",
      "train loss:0.1377709415308489\n",
      "train loss:0.1615159208530019\n",
      "train loss:0.1446106930857933\n",
      "train loss:0.23079540035538035\n",
      "train loss:0.1439964558565822\n",
      "train loss:0.07601760594407374\n",
      "train loss:0.1834694006396057\n",
      "train loss:0.17253795704285188\n",
      "train loss:0.1596980308894565\n",
      "train loss:0.19257692098786616\n",
      "=== epoch:7, train acc:0.933, test acc:0.913 ===\n",
      "train loss:0.3352986008064503\n",
      "train loss:0.13088075115285397\n",
      "train loss:0.17158662505120575\n",
      "train loss:0.21774195660752282\n",
      "train loss:0.21278499397274497\n",
      "train loss:0.11056645892963363\n",
      "train loss:0.2402899969980099\n",
      "train loss:0.1026644118004368\n",
      "train loss:0.11205122355665076\n",
      "train loss:0.1886439990268418\n",
      "train loss:0.157707767222707\n",
      "train loss:0.22669770459852365\n",
      "train loss:0.21196218621664836\n",
      "train loss:0.21185835987631368\n",
      "train loss:0.28403451031084165\n",
      "train loss:0.17867848157630398\n",
      "train loss:0.15364823376275438\n",
      "train loss:0.3208579842940089\n",
      "train loss:0.13790908053936357\n",
      "train loss:0.17717367032486694\n",
      "train loss:0.11092847013169005\n",
      "train loss:0.17135601556318916\n",
      "train loss:0.185991863359449\n",
      "train loss:0.19568705824676716\n",
      "train loss:0.046820407741939685\n",
      "train loss:0.09207127784010877\n",
      "train loss:0.233178393214106\n",
      "train loss:0.08721633683215614\n",
      "train loss:0.16420696460741996\n",
      "train loss:0.14724418137503367\n",
      "train loss:0.13456727770350865\n",
      "train loss:0.14183493298480487\n",
      "train loss:0.16201133094623132\n",
      "train loss:0.1030520729782181\n",
      "train loss:0.2119166268364948\n",
      "train loss:0.22053295103366463\n",
      "train loss:0.11895952984745632\n",
      "train loss:0.15276536316600273\n",
      "train loss:0.12600221046182716\n",
      "train loss:0.07928538881122503\n",
      "train loss:0.10339847454762686\n",
      "train loss:0.1240318784603637\n",
      "train loss:0.12869694813179827\n",
      "train loss:0.08792458366830054\n",
      "train loss:0.1279670963335266\n",
      "train loss:0.21828366965187995\n",
      "train loss:0.09760852975877116\n",
      "train loss:0.2007949497955924\n",
      "train loss:0.1911111469122515\n",
      "train loss:0.17702079447144087\n",
      "=== epoch:8, train acc:0.942, test acc:0.928 ===\n",
      "train loss:0.0763863216566728\n",
      "train loss:0.17013115839899653\n",
      "train loss:0.09656990091890989\n",
      "train loss:0.12038243086020954\n",
      "train loss:0.1315307827040992\n",
      "train loss:0.08668026653346372\n",
      "train loss:0.18374669801184113\n",
      "train loss:0.09467987899655504\n",
      "train loss:0.09131566962189659\n",
      "train loss:0.17336669423893164\n",
      "train loss:0.13722288967133078\n",
      "train loss:0.15442987402154595\n",
      "train loss:0.13293316204819658\n",
      "train loss:0.11349096576046072\n",
      "train loss:0.26001295731334867\n",
      "train loss:0.11760442768885522\n",
      "train loss:0.14580315883056275\n",
      "train loss:0.23855653264279147\n",
      "train loss:0.14905323381389027\n",
      "train loss:0.08281526247249009\n",
      "train loss:0.0974865321665259\n",
      "train loss:0.18787932009058209\n",
      "train loss:0.19286428164970015\n",
      "train loss:0.0848725207582386\n",
      "train loss:0.16374000011239886\n",
      "train loss:0.24089765318013634\n",
      "train loss:0.08663329537738715\n",
      "train loss:0.107883747618375\n",
      "train loss:0.1340796108226886\n",
      "train loss:0.04865180623728747\n",
      "train loss:0.06890545306444151\n",
      "train loss:0.16917853424996795\n",
      "train loss:0.1175462178842536\n",
      "train loss:0.19739738051568068\n",
      "train loss:0.1210523607027742\n",
      "train loss:0.1445891190798915\n",
      "train loss:0.08861983334637827\n",
      "train loss:0.16095461006604292\n",
      "train loss:0.10918240794933018\n",
      "train loss:0.10033604764953807\n",
      "train loss:0.0756036844549162\n",
      "train loss:0.05563154437835079\n",
      "train loss:0.10641432750691944\n",
      "train loss:0.1576331697519493\n",
      "train loss:0.12117948336596919\n",
      "train loss:0.18876174068704468\n",
      "train loss:0.13934483999508046\n",
      "train loss:0.10191630330375019\n",
      "train loss:0.127005071013172\n",
      "train loss:0.08674180574505815\n",
      "=== epoch:9, train acc:0.951, test acc:0.925 ===\n",
      "train loss:0.07150819098448476\n",
      "train loss:0.09140880549027226\n",
      "train loss:0.06167376587826946\n",
      "train loss:0.26740755945019773\n",
      "train loss:0.1497990044055457\n",
      "train loss:0.11480459913255558\n",
      "train loss:0.2267049090821827\n",
      "train loss:0.09968556218094349\n",
      "train loss:0.11233832406791207\n",
      "train loss:0.10253757745455942\n",
      "train loss:0.0845478785256041\n",
      "train loss:0.08740700441764586\n",
      "train loss:0.068221819577195\n",
      "train loss:0.12355486259919543\n",
      "train loss:0.26524091272544387\n",
      "train loss:0.22312641588320467\n",
      "train loss:0.08950349269334124\n",
      "train loss:0.0846559893200682\n",
      "train loss:0.12527920986925126\n",
      "train loss:0.05334168587439972\n",
      "train loss:0.12947892016532478\n",
      "train loss:0.23123154035874127\n",
      "train loss:0.1089986819735782\n",
      "train loss:0.2253602516349489\n",
      "train loss:0.10093005331073622\n",
      "train loss:0.09568804158851256\n",
      "train loss:0.06299284535580674\n",
      "train loss:0.09689819957120463\n",
      "train loss:0.060893731818784526\n",
      "train loss:0.13308371427678023\n",
      "train loss:0.1664059717251204\n",
      "train loss:0.1262222220795497\n",
      "train loss:0.12745825633323488\n",
      "train loss:0.12904377963112068\n",
      "train loss:0.1095811400082314\n",
      "train loss:0.15230290697744844\n",
      "train loss:0.09418699044423524\n",
      "train loss:0.1178688937591875\n",
      "train loss:0.07111795249960987\n",
      "train loss:0.0757873255614018\n",
      "train loss:0.14748062288256292\n",
      "train loss:0.08563462386226486\n",
      "train loss:0.08209767616775547\n",
      "train loss:0.1021449941708468\n",
      "train loss:0.12336861145800809\n",
      "train loss:0.05746712732899465\n",
      "train loss:0.1204591929751381\n",
      "train loss:0.136924093711924\n",
      "train loss:0.12667228700721\n",
      "train loss:0.05159806653340248\n",
      "=== epoch:10, train acc:0.954, test acc:0.935 ===\n",
      "train loss:0.09738069088447407\n",
      "train loss:0.11384056502095952\n",
      "train loss:0.1564292766589036\n",
      "train loss:0.04566045573537233\n",
      "train loss:0.1253618164456505\n",
      "train loss:0.09146476173339056\n",
      "train loss:0.08378220999427562\n",
      "train loss:0.06588310978415308\n",
      "train loss:0.15059772689606954\n",
      "train loss:0.11281056897354119\n",
      "train loss:0.14312325299528034\n",
      "train loss:0.06640940133276091\n",
      "train loss:0.12641020881400605\n",
      "train loss:0.034548932645400106\n",
      "train loss:0.04486709107274653\n",
      "train loss:0.10184537396744373\n",
      "train loss:0.12297477793226527\n",
      "train loss:0.07947732784247767\n",
      "train loss:0.07740829000910346\n",
      "train loss:0.17635532454241468\n",
      "train loss:0.04241283956065875\n",
      "train loss:0.03944395785227959\n",
      "train loss:0.23063665419133858\n",
      "train loss:0.055804723171092085\n",
      "train loss:0.14848940429701585\n",
      "train loss:0.08017974193443188\n",
      "train loss:0.09345915348240841\n",
      "train loss:0.26551621126274855\n",
      "train loss:0.0919502897307142\n",
      "train loss:0.11397497748131326\n",
      "train loss:0.0844725651005312\n",
      "train loss:0.07640265624704491\n",
      "train loss:0.223021614554457\n",
      "train loss:0.23963644505891743\n",
      "train loss:0.08908331431764883\n",
      "train loss:0.07144932935072373\n",
      "train loss:0.29145109722205853\n",
      "train loss:0.07779713084431487\n",
      "train loss:0.15962247637651758\n",
      "train loss:0.08311213697097994\n",
      "train loss:0.1059707460674742\n",
      "train loss:0.1885440399213386\n",
      "train loss:0.11955560643446955\n",
      "train loss:0.09161193045265704\n",
      "train loss:0.05481959955544093\n",
      "train loss:0.10957903641234053\n",
      "train loss:0.11047270282540989\n",
      "train loss:0.1418773309670804\n",
      "train loss:0.08899705073061291\n",
      "train loss:0.10233832696908379\n",
      "=== epoch:11, train acc:0.962, test acc:0.941 ===\n",
      "train loss:0.09345422910403807\n",
      "train loss:0.07379179459116282\n",
      "train loss:0.14071356514356434\n",
      "train loss:0.1540514642077133\n",
      "train loss:0.07141600945567571\n",
      "train loss:0.07399999660360508\n",
      "train loss:0.10253791291395772\n",
      "train loss:0.1818993726959031\n",
      "train loss:0.17421234794310453\n",
      "train loss:0.1686016368898898\n",
      "train loss:0.11245009428179319\n",
      "train loss:0.10040687241201798\n",
      "train loss:0.1453558648795665\n",
      "train loss:0.08501037949743306\n",
      "train loss:0.04812507023023861\n",
      "train loss:0.1291386926681103\n",
      "train loss:0.06932507539282758\n",
      "train loss:0.07108115342101971\n",
      "train loss:0.11786518924085171\n",
      "train loss:0.10707388761437008\n",
      "train loss:0.09847624384046984\n",
      "train loss:0.08186169112787622\n",
      "train loss:0.08778149801154761\n",
      "train loss:0.06847717669087798\n",
      "train loss:0.1507832344118248\n",
      "train loss:0.09602396455616362\n",
      "train loss:0.13043635277321064\n",
      "train loss:0.21004553889352312\n",
      "train loss:0.12755379169556025\n",
      "train loss:0.08364313750284767\n",
      "train loss:0.05690226959367489\n",
      "train loss:0.09520772966930695\n",
      "train loss:0.0646835319666093\n",
      "train loss:0.05444581611025827\n",
      "train loss:0.05056910730196166\n",
      "train loss:0.11731099402856772\n",
      "train loss:0.21802251341837808\n",
      "train loss:0.0934882571550818\n",
      "train loss:0.057536766998996\n",
      "train loss:0.06031765479874851\n",
      "train loss:0.22774045211182214\n",
      "train loss:0.06624749916847193\n",
      "train loss:0.07304467935606142\n",
      "train loss:0.08461910531966087\n",
      "train loss:0.07124776537250507\n",
      "train loss:0.06936715718714752\n",
      "train loss:0.08277648269049796\n",
      "train loss:0.09464682480078931\n",
      "train loss:0.11102199787127777\n",
      "train loss:0.07167505261340963\n",
      "=== epoch:12, train acc:0.97, test acc:0.942 ===\n",
      "train loss:0.10302614138716436\n",
      "train loss:0.06059999496618326\n",
      "train loss:0.14199618907222983\n",
      "train loss:0.06711228543442754\n",
      "train loss:0.09783212471572732\n",
      "train loss:0.1402170827342381\n",
      "train loss:0.027253343668358444\n",
      "train loss:0.12209390821782039\n",
      "train loss:0.0530882353543508\n",
      "train loss:0.09448873652653898\n",
      "train loss:0.12367589967422811\n",
      "train loss:0.0474427658835109\n",
      "train loss:0.024290019872436622\n",
      "train loss:0.030805191547617964\n",
      "train loss:0.09945378943593802\n",
      "train loss:0.07620180089377787\n",
      "train loss:0.05230905106598776\n",
      "train loss:0.042901884707427104\n",
      "train loss:0.05863239417093871\n",
      "train loss:0.11017306014829047\n",
      "train loss:0.055257000325773145\n",
      "train loss:0.05150913687267912\n",
      "train loss:0.1163202623345351\n",
      "train loss:0.07023077820729365\n",
      "train loss:0.14703815007756207\n",
      "train loss:0.07544140459043776\n",
      "train loss:0.11872057690039062\n",
      "train loss:0.12028561243766413\n",
      "train loss:0.04031274608382276\n",
      "train loss:0.04986712677052647\n",
      "train loss:0.06925008673882194\n",
      "train loss:0.07040422552102452\n",
      "train loss:0.11904243861652886\n",
      "train loss:0.057689461964036196\n",
      "train loss:0.07039495383024141\n",
      "train loss:0.1089471194687917\n",
      "train loss:0.051144063571124374\n",
      "train loss:0.07341406307085188\n",
      "train loss:0.06368211215747\n",
      "train loss:0.04915768071607013\n",
      "train loss:0.10569317738383857\n",
      "train loss:0.044009859100422356\n",
      "train loss:0.07170312598030605\n",
      "train loss:0.08643044528601093\n",
      "train loss:0.04705867284576108\n",
      "train loss:0.1433296295152667\n",
      "train loss:0.13567647122893647\n",
      "train loss:0.05935276068353058\n",
      "train loss:0.08572553308751818\n",
      "train loss:0.04944960384985993\n",
      "=== epoch:13, train acc:0.969, test acc:0.944 ===\n",
      "train loss:0.07759553829190352\n",
      "train loss:0.09051515619779218\n",
      "train loss:0.048324825793775276\n",
      "train loss:0.038273130379821885\n",
      "train loss:0.045605155371336074\n",
      "train loss:0.054007401920160465\n",
      "train loss:0.10192071350465287\n",
      "train loss:0.09613052923772286\n",
      "train loss:0.02987366645767831\n",
      "train loss:0.09087416650423612\n",
      "train loss:0.037297554288240366\n",
      "train loss:0.048949127114107975\n",
      "train loss:0.03302226350236401\n",
      "train loss:0.08854701151679623\n",
      "train loss:0.109371875624758\n",
      "train loss:0.05504960519934194\n",
      "train loss:0.03865550999432034\n",
      "train loss:0.03899436934073185\n",
      "train loss:0.0927895394407429\n",
      "train loss:0.05577606169309171\n",
      "train loss:0.09452269505380714\n",
      "train loss:0.048286357320163634\n",
      "train loss:0.13006943056791945\n",
      "train loss:0.0654044653259464\n",
      "train loss:0.06062387036060144\n",
      "train loss:0.05492065688479505\n",
      "train loss:0.07409056730930247\n",
      "train loss:0.07989198323278857\n",
      "train loss:0.04656445860375372\n",
      "train loss:0.041180541009982495\n",
      "train loss:0.056491746419493116\n",
      "train loss:0.021583193169074075\n",
      "train loss:0.06815227712329197\n",
      "train loss:0.03841927832859367\n",
      "train loss:0.058740336339629114\n",
      "train loss:0.08299112699914804\n",
      "train loss:0.06482641477746247\n",
      "train loss:0.15316722239258296\n",
      "train loss:0.052995901214484255\n",
      "train loss:0.0393106069741932\n",
      "train loss:0.030788374395072714\n",
      "train loss:0.13617647283864887\n",
      "train loss:0.03154099627872717\n",
      "train loss:0.10467122857721031\n",
      "train loss:0.05220918699616711\n",
      "train loss:0.05393131474002802\n",
      "train loss:0.06338512686134293\n",
      "train loss:0.03394475987648289\n",
      "train loss:0.02321914630060847\n",
      "train loss:0.049901395909674615\n",
      "=== epoch:14, train acc:0.971, test acc:0.942 ===\n",
      "train loss:0.12400410182984169\n",
      "train loss:0.10898360338984425\n",
      "train loss:0.04614695473409631\n",
      "train loss:0.06280069714008565\n",
      "train loss:0.054849756666457535\n",
      "train loss:0.057115580096218886\n",
      "train loss:0.07154394665624682\n",
      "train loss:0.08930115262075289\n",
      "train loss:0.03427863660632042\n",
      "train loss:0.05216234297669297\n",
      "train loss:0.038453475108542634\n",
      "train loss:0.03704644278936978\n",
      "train loss:0.06195203809425513\n",
      "train loss:0.06410284780469548\n",
      "train loss:0.059947820535733214\n",
      "train loss:0.07660832936854224\n",
      "train loss:0.0446887165601599\n",
      "train loss:0.04551910244045953\n",
      "train loss:0.026380327038477716\n",
      "train loss:0.04018838533879339\n",
      "train loss:0.022127862628546806\n",
      "train loss:0.08769989468313945\n",
      "train loss:0.0614058846974885\n",
      "train loss:0.031694565296743686\n",
      "train loss:0.04736228056993643\n",
      "train loss:0.07553040946939275\n",
      "train loss:0.01630546176050003\n",
      "train loss:0.050591249933090145\n",
      "train loss:0.05323152419458958\n",
      "train loss:0.04144355722781104\n",
      "train loss:0.03170528539676738\n",
      "train loss:0.039323393415477484\n",
      "train loss:0.0804806219739111\n",
      "train loss:0.030278900035865367\n",
      "train loss:0.0775004619233051\n",
      "train loss:0.043373678105436665\n",
      "train loss:0.0537232658604675\n",
      "train loss:0.13544815567839769\n",
      "train loss:0.02351855405323593\n",
      "train loss:0.037697895335875004\n",
      "train loss:0.036708729405836\n",
      "train loss:0.06253580114862949\n",
      "train loss:0.07277959430203454\n",
      "train loss:0.04504637431434821\n",
      "train loss:0.07980660459369904\n",
      "train loss:0.09254816421804911\n",
      "train loss:0.07866682170315814\n",
      "train loss:0.14206154396809206\n",
      "train loss:0.03994547088973881\n",
      "train loss:0.03347073021338599\n",
      "=== epoch:15, train acc:0.979, test acc:0.951 ===\n",
      "train loss:0.02954626487793486\n",
      "train loss:0.04162360123616927\n",
      "train loss:0.09903878079757676\n",
      "train loss:0.10656247052475339\n",
      "train loss:0.03901294225827337\n",
      "train loss:0.08266090376700076\n",
      "train loss:0.14879602033816228\n",
      "train loss:0.048063983427707305\n",
      "train loss:0.029534073135952736\n",
      "train loss:0.0621025999835763\n",
      "train loss:0.08201620675162838\n",
      "train loss:0.0717354052960034\n",
      "train loss:0.05872081180679567\n",
      "train loss:0.11735447247555612\n",
      "train loss:0.05782751628133002\n",
      "train loss:0.02591467629757257\n",
      "train loss:0.046291660767177015\n",
      "train loss:0.03831809521541877\n",
      "train loss:0.041940815020376644\n",
      "train loss:0.04446301987530385\n",
      "train loss:0.05896715164954438\n",
      "train loss:0.037321170896652746\n",
      "train loss:0.04995188546672986\n",
      "train loss:0.05227745658213426\n",
      "train loss:0.08636561445141963\n",
      "train loss:0.05918361634324237\n",
      "train loss:0.04502890988118052\n",
      "train loss:0.028837717468393628\n",
      "train loss:0.05305257772711181\n",
      "train loss:0.07491423969788431\n",
      "train loss:0.031042518589005762\n",
      "train loss:0.022370677297333795\n",
      "train loss:0.04774941821493279\n",
      "train loss:0.045998184647892376\n",
      "train loss:0.020342812537221925\n",
      "train loss:0.05283687462725584\n",
      "train loss:0.0717532075722703\n",
      "train loss:0.02922398700701017\n",
      "train loss:0.09527371514865911\n",
      "train loss:0.0779815752848458\n",
      "train loss:0.02665131657460734\n",
      "train loss:0.034831786276826246\n",
      "train loss:0.0342552187094509\n",
      "train loss:0.0387660294676581\n",
      "train loss:0.15059633587338828\n",
      "train loss:0.13396804798863884\n",
      "train loss:0.1265194600396254\n",
      "train loss:0.03468187898317458\n",
      "train loss:0.045444470609174364\n",
      "train loss:0.013948522469527129\n",
      "=== epoch:16, train acc:0.973, test acc:0.952 ===\n",
      "train loss:0.031267665212697075\n",
      "train loss:0.10000479857885136\n",
      "train loss:0.03510880199208842\n",
      "train loss:0.029494591217066195\n",
      "train loss:0.023646535305948672\n",
      "train loss:0.012589701716037436\n",
      "train loss:0.02788615469736622\n",
      "train loss:0.04781805038756224\n",
      "train loss:0.06388339637372503\n",
      "train loss:0.0800672446294868\n",
      "train loss:0.02141663184998865\n",
      "train loss:0.028627645825221502\n",
      "train loss:0.011888598514168314\n",
      "train loss:0.053232699990499566\n",
      "train loss:0.023386789049451507\n",
      "train loss:0.04454738339657025\n",
      "train loss:0.02237042651192782\n",
      "train loss:0.034420263033974\n",
      "train loss:0.03773736764496933\n",
      "train loss:0.0364052272404239\n",
      "train loss:0.017059475161283214\n",
      "train loss:0.06623814851612717\n",
      "train loss:0.04261337636020899\n",
      "train loss:0.01736425281268911\n",
      "train loss:0.03127049530146244\n",
      "train loss:0.03903385384342784\n",
      "train loss:0.07085852498277842\n",
      "train loss:0.0498611385908891\n",
      "train loss:0.10818350791483869\n",
      "train loss:0.06691156284076992\n",
      "train loss:0.029178976353164604\n",
      "train loss:0.0338384520017186\n",
      "train loss:0.03177967653397148\n",
      "train loss:0.055520988337459824\n",
      "train loss:0.03275669685627835\n",
      "train loss:0.036486002096217406\n",
      "train loss:0.04202347084608819\n",
      "train loss:0.029876271197214383\n",
      "train loss:0.02133512886030617\n",
      "train loss:0.05336919559084751\n",
      "train loss:0.1747929038312833\n",
      "train loss:0.061075220143011366\n",
      "train loss:0.0395169869892715\n",
      "train loss:0.040671918827053864\n",
      "train loss:0.021533692910958347\n",
      "train loss:0.016799257611252425\n",
      "train loss:0.026040907008023882\n",
      "train loss:0.05215276275908456\n",
      "train loss:0.01643207191458783\n",
      "train loss:0.026073931573728896\n",
      "=== epoch:17, train acc:0.977, test acc:0.942 ===\n",
      "train loss:0.05602139772387346\n",
      "train loss:0.03370178843657933\n",
      "train loss:0.01993771767218391\n",
      "train loss:0.10544620307602574\n",
      "train loss:0.06323268911868457\n",
      "train loss:0.04573332985463113\n",
      "train loss:0.05475222644934691\n",
      "train loss:0.02289621870804546\n",
      "train loss:0.03600636868069417\n",
      "train loss:0.042264385916684584\n",
      "train loss:0.08245483000294217\n",
      "train loss:0.028519380514551905\n",
      "train loss:0.02729248386147498\n",
      "train loss:0.034953723780972744\n",
      "train loss:0.01903415681724463\n",
      "train loss:0.06246741014961727\n",
      "train loss:0.060703615601686724\n",
      "train loss:0.029487520997051923\n",
      "train loss:0.03606467069248539\n",
      "train loss:0.07633624206862273\n",
      "train loss:0.05402240990234455\n",
      "train loss:0.038747352420387224\n",
      "train loss:0.06168662863325802\n",
      "train loss:0.049640666798301404\n",
      "train loss:0.028795293835269985\n",
      "train loss:0.036324000989919815\n",
      "train loss:0.06987086751352642\n",
      "train loss:0.024678164406026076\n",
      "train loss:0.04013288132216451\n",
      "train loss:0.06506465350972512\n",
      "train loss:0.06059282996135727\n",
      "train loss:0.04199759171341846\n",
      "train loss:0.04644865434146106\n",
      "train loss:0.020275929694425102\n",
      "train loss:0.03182788614172328\n",
      "train loss:0.03373071136445256\n",
      "train loss:0.026397097454878754\n",
      "train loss:0.0753640591837807\n",
      "train loss:0.03064448704876859\n",
      "train loss:0.021539128547079178\n",
      "train loss:0.03217174135606086\n",
      "train loss:0.04627609832712893\n",
      "train loss:0.01573056229373093\n",
      "train loss:0.033298691860583145\n",
      "train loss:0.0122433668354583\n",
      "train loss:0.04752873048483268\n",
      "train loss:0.041785482192213265\n",
      "train loss:0.07208137600431386\n",
      "train loss:0.03994461385810362\n",
      "train loss:0.025115177244357047\n",
      "=== epoch:18, train acc:0.977, test acc:0.943 ===\n",
      "train loss:0.043213252004561296\n",
      "train loss:0.024276539830595845\n",
      "train loss:0.020582457491758507\n",
      "train loss:0.03484447699684855\n",
      "train loss:0.03301455894615224\n",
      "train loss:0.029834221306211092\n",
      "train loss:0.05610454773167935\n",
      "train loss:0.04308095782416022\n",
      "train loss:0.024837370370860562\n",
      "train loss:0.0278536415998193\n",
      "train loss:0.043223409302917105\n",
      "train loss:0.025692225038556976\n",
      "train loss:0.02355609669330216\n",
      "train loss:0.03406165859260499\n",
      "train loss:0.01836450752043857\n",
      "train loss:0.0180379958136193\n",
      "train loss:0.02909779127530687\n",
      "train loss:0.02966202007369077\n",
      "train loss:0.045460921836440245\n",
      "train loss:0.03414220933803063\n",
      "train loss:0.017797052500443857\n",
      "train loss:0.015488970564493916\n",
      "train loss:0.017612799175245403\n",
      "train loss:0.04229514879557078\n",
      "train loss:0.00866435538058172\n",
      "train loss:0.03490688435289432\n",
      "train loss:0.05670815452529203\n",
      "train loss:0.05072877068944566\n",
      "train loss:0.06955210129583639\n",
      "train loss:0.05069822337074873\n",
      "train loss:0.01351009322477604\n",
      "train loss:0.038706462461671066\n",
      "train loss:0.06196720466422728\n",
      "train loss:0.021002768234905926\n",
      "train loss:0.0209667982658042\n",
      "train loss:0.018472755140081554\n",
      "train loss:0.08123356303374102\n",
      "train loss:0.040774110774587646\n",
      "train loss:0.034484469314827335\n",
      "train loss:0.055874837935080836\n",
      "train loss:0.03684828318461678\n",
      "train loss:0.011373715969392397\n",
      "train loss:0.03595068116779862\n",
      "train loss:0.021179401383935686\n",
      "train loss:0.05452758582698429\n",
      "train loss:0.026014633561689187\n",
      "train loss:0.008691900517546522\n",
      "train loss:0.047115024734917424\n",
      "train loss:0.011151591327599986\n",
      "train loss:0.03063933505033789\n",
      "=== epoch:19, train acc:0.989, test acc:0.951 ===\n",
      "train loss:0.01960779480709141\n",
      "train loss:0.03155163371966498\n",
      "train loss:0.012782914505686867\n",
      "train loss:0.013725832294543409\n",
      "train loss:0.02260617724309613\n",
      "train loss:0.05889022897107514\n",
      "train loss:0.028809987207849636\n",
      "train loss:0.03558907838100818\n",
      "train loss:0.02889399716437209\n",
      "train loss:0.013954953825100895\n",
      "train loss:0.017499348568498507\n",
      "train loss:0.02842816859142399\n",
      "train loss:0.018064398926847417\n",
      "train loss:0.021109175816729175\n",
      "train loss:0.019839844954891214\n",
      "train loss:0.01884301403728129\n",
      "train loss:0.013870868923253505\n",
      "train loss:0.02658578627880614\n",
      "train loss:0.018373011225820475\n",
      "train loss:0.031847757651411145\n",
      "train loss:0.019272451314292106\n",
      "train loss:0.04024364621003307\n",
      "train loss:0.021523419093802624\n",
      "train loss:0.008780899098873309\n",
      "train loss:0.032545988382934196\n",
      "train loss:0.02125082401175726\n",
      "train loss:0.02248893362272017\n",
      "train loss:0.010649039050934365\n",
      "train loss:0.07120409422258604\n",
      "train loss:0.015450724731079692\n",
      "train loss:0.018312090710404193\n",
      "train loss:0.03206894149796667\n",
      "train loss:0.022214511502301356\n",
      "train loss:0.01637436052626595\n",
      "train loss:0.03190790993461634\n",
      "train loss:0.02641228640300874\n",
      "train loss:0.04436799258967135\n",
      "train loss:0.04248780729358112\n",
      "train loss:0.02745580796965381\n",
      "train loss:0.06149866016617512\n",
      "train loss:0.020928154425277276\n",
      "train loss:0.017862298054113455\n",
      "train loss:0.022972924006256164\n",
      "train loss:0.032323431333136796\n",
      "train loss:0.01852298185494102\n",
      "train loss:0.039043604969004456\n",
      "train loss:0.050324072688399626\n",
      "train loss:0.03410488154682513\n",
      "train loss:0.021745389644510723\n",
      "train loss:0.03739079904753409\n",
      "=== epoch:20, train acc:0.989, test acc:0.956 ===\n",
      "train loss:0.011646766623187157\n",
      "train loss:0.035646341522403306\n",
      "train loss:0.008305241074342952\n",
      "train loss:0.008231386085415822\n",
      "train loss:0.017322324206630647\n",
      "train loss:0.008369332278584692\n",
      "train loss:0.028228443533940405\n",
      "train loss:0.016796808203074\n",
      "train loss:0.01808370178894822\n",
      "train loss:0.01703924327545352\n",
      "train loss:0.011588391538370608\n",
      "train loss:0.02235564540956968\n",
      "train loss:0.025385557941781118\n",
      "train loss:0.01779492787183519\n",
      "train loss:0.02524883807096821\n",
      "train loss:0.0353063163232345\n",
      "train loss:0.013172209169398819\n",
      "train loss:0.011148949857541517\n",
      "train loss:0.022869366845356724\n",
      "train loss:0.05074125544333552\n",
      "train loss:0.021799522423135317\n",
      "train loss:0.023131511594202996\n",
      "train loss:0.012433999366629827\n",
      "train loss:0.009065218407120067\n",
      "train loss:0.029105526930329383\n",
      "train loss:0.04784554656432704\n",
      "train loss:0.015247230675775623\n",
      "train loss:0.051675090567173836\n",
      "train loss:0.011407305577967557\n",
      "train loss:0.012382268578080247\n",
      "train loss:0.021799170960109828\n",
      "train loss:0.015693972015824884\n",
      "train loss:0.03095252551799077\n",
      "train loss:0.04394032494050457\n",
      "train loss:0.018223110485337476\n",
      "train loss:0.02139750670169771\n",
      "train loss:0.0585320740468726\n",
      "train loss:0.010939526644459019\n",
      "train loss:0.026991752885292618\n",
      "train loss:0.02785023758685588\n",
      "train loss:0.01093038693730899\n",
      "train loss:0.017876359252698214\n",
      "train loss:0.015648216609458036\n",
      "train loss:0.0536457879417549\n",
      "train loss:0.013615233098604098\n",
      "train loss:0.03143108004710269\n",
      "train loss:0.030657133509245864\n",
      "train loss:0.025921952626706487\n",
      "train loss:0.016755268298682468\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.955\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SimpleConvNet을 MNIST 데이터셋으로 학습\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"/Users/krc/Downloads/deep-learning-from-scratch-master/ch07\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcEAAAEgCAYAAADMo8jPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAbmklEQVR4nO3ca3BV5dn/8WvnfD4nJIFAooAQQaCAnRZPdaRSammZjm0tOuiLnqjOFFvetE470xmqg606HdpO6Yx1qBSrKL6gUgQ5VCpIQRoR5BAwAUpDssn5TLLX/0WaPdv+sfdv9cE+j7m/n1dL53df3Guvtfe1d2bWFQmCwAAA8FHS//YGAAD430ITBAB4iyYIAPAWTRAA4C2aIADAWzRBAIC3UsKE8/Pzg3Hjxjlzra2tck01m56eLtdMTU11Zvr7+21wcDBiZpaSkhKkpaVdlbqjkpOTpVxpaalcMzc3V8odOnQoGgRBaXJycqDsORaLyXtQRSKRq15zcHAwGgRBqZlZXl5eUFZW5lwT5poNDw9Luf7+frmmku3q6rK+vr6ImVlOTk5QVFTkXNPd3S3vQb0Wg4ODcs2kJO37c2dnZzQIgtLCwsKgsrLSmb906ZK8h97eXikX5jGwlBTtI7G9vT1+L6ampgbK51OY94S6jzA1lfd5X1+fDQwMRMzMSkpKgurqauea5uZmeQ8dHR1SLsz7Vv1cbGhoiF+zRKGa4Lhx4+wXv/iFM7dx40a55nPPPSflrr32WrlmeXm5M/Pmm2/Gj9PS0mzKlCnONcqbeFReXp6UW7FihVzz1ltvlXKRSKTRbORGmjhxojMf5kNd/UBRvwSY6W/khoaGxtHjsrIyW7NmjXNNmGvW3t4u5err6+WaR48edWZefPHF+HFRUZGtWrXKueYvf/mLvAf1A+X8+fNyzczMTCm3devWRrOR6/D73//emX/22WflPdTV1Um5MM1d+QJiZrZ58+b4vZienm4zZsxwrlFfMzOzwsJCKac2SzPtddi9e3f8uLq62g4ePOhcs3btWnkPW7ZskXJh3re33XablFu+fHnjlf4/fw4FAHiLJggA8BZNEADgLZogAMBbNEEAgLdoggAAb9EEAQDeogkCALwV6mH5vLw8W7hwoTP3q1/9Sq6pTp44e/asXFOZcpD475aUlNjXvvY155owDxOrD15/mIaGhqQJHH19fXJNdbrMwMCAXPM/0dvba2+//bYzt2PHDrnmrl27pNznPvc5uebMmTOdmVdeeSV+XFZWZg899JBzTZgHwN944w0pd/nyZbnm5z//eSm3detWMxt5UHzWrFnOfJiH5bu6uqTcxYsX5Zrz58+Xs6PU91mY11f9rAkzMWbChAnOTOL7u7OzU3r/ZGRkyHtQJwLV1NTINRcvXixnr4RfggAAb9EEAQDeogkCALxFEwQAeIsmCADwFk0QAOAtmiAAwFs0QQCAt2iCAABv0QQBAN4KNTatvb3dXnrpJWfu2LFjck11DFdtba1cc+rUqc7MX//61/hxfn6+3Xnnnc41f/7zn+U9qOO6XnvtNbnmnj175KyZWVZWljSqKsyIt+7ubikXZpxTf3+/lEscndfa2mobN250rjl58qS8j4KCAimnjn4yM1uzZo0z8/TTT8ePm5ubbe3atc41P/3pT+U9qPt94IEH5JrqqKqVK1eamVlTU5M9/vjjznyYezzxPfzvKOPCRmVlZcnZUenp6TZlyhRnrqmpSa5ZXl4u5fbt2yfXzM3NdWaGh4fjx7FYzHp6epxrDhw4IO8hsf6/o/YFs3AjBK+EX4IAAG/RBAEA3qIJAgC8RRMEAHiLJggA8BZNEADgLZogAMBbNEEAgLdoggAAb4WaGBONRt833eKDnDhx4j/e0AdJSdG32tDQ4MwMDAzEj9va2mzTpk3ONdu3b5f3sGvXLim3ZMkSuWZvb6+cNTOrqKiwH/7wh85cUpL+XSgzM1PKhZlCo04JWb16dfw4NTVVmqoRZlrI+fPnpdypU6fkms8995wz09raGj8OgkCaoNPc3CzvQZ2+sWHDBrlmdna2nDUz6+vrs8OHDztzYV5b9TNhxowZcs28vDw5O6q4uNjuu+8+Z66srEyuuXv3bikXZmLMW2+9JWfNRq5ZXV2dMxfmXlAmc5mZPfbYY3LNyZMny9kr4ZcgAMBbNEEAgLdoggAAb9EEAQDeogkCALxFEwQAeIsmCADwFk0QAOAtmiAAwFs0QQCAt0KNTevq6pLG+XzqU5+Sa6ojdzo7O+WafX19zkziKKlIJCKNA5szZ468h5KSEikXZhTa0qVLpdyOHTvMbOS1nTdvnjOfmpoq7yE9PV3KJY6lc1HvgcSxafn5+bZo0SLnmuuuu07eR3d3t5T705/+JNcMOzYtLy9POq8wYwTPnj0r5ZS9jgozts1s5H44c+aMMxfmetXW1kq5j33sY3LNBx98UMp973vfix8XFRXZPffc41yjjCAbpV7f6upquebx48edmcT3bU5Oji1YsMC5JsyouWPHjkm5NWvWyDVvvPFGOXsl/BIEAHiLJggA8BZNEADgLZogAMBbNEEAgLdoggAAb9EEAQDeogkCALxFEwQAeCsSBIEejkRazKzxw9vOf9WkIAhKzcbceZn989zG6nmZjblrNlbPy4x78aNmrJ6XWcK5JQrVBAEAGEv4cygAwFs0QQCAt2iCAABv0QQBAN6iCQIAvEUTBAB4iyYIAPAWTRAA4C2aIADAWzRBAIC3aIIAAG/RBAEA3qIJAgC8RRMEAHiLJggA8BZNEADgLZogAMBbNEEAgLdoggAAb6WECaenpwdZWVnO3MDAgFwzCAIpl5GRIdcsLi52Zpqbm62zszNiZpaTkxMUFRU512RmZsp7UF4nM7Pk5GS55tDQkJSrq6uLBkFQql6vSCQi76G/v1/K5eTkyDVVLS0t0SAISs3MCgoKgvLycuca9TUzM2tra7uqOTOzpCT398xYLGaxWCxiZpaZmRnk5eU516jXwcwsLS1Nyin/7qjCwkIpd+jQoWgQBKUlJSVBdXW1M9/e3i7vobW1Vcp1dHTINdX3eE9PT/xezM3NDZTPnN7eXnkfw8PDUi47O1uuqZxbU1OTdXR0RMzMMjIygtzcXOeay5cvy3tQ3g9hpaamSrnm5ub4NUsUqglmZWXZ7bff7szV19fLNWOxmJSbPHmyXPP+++93Zr773e/Gj4uKimzVqlXONTfccIO8h9mzZ0u5/Px8uealS5ekXElJSaPZyPW67bbbnPkwXzCOHTsm5RYsWCDXVL8IrF27tnH0uLy83J5++mnnmubmZnkfmzZtuqo5M+2Dp6urK36cl5dn99xzj3PNyZMn5T2MHz9eyt1xxx1yzS9/+ctSLhKJNJqZVVdX28GDB535l19+Wd7DH/7wByn3yiuvyDVra2ul3P79++P3YnFxsT3yyCPONYcOHZL30dPTI+Xmzp0r15w1a5Yz841vfCN+nJuba1/84heda5qamuQ9qF/I1B9HZmZVVVVS7sknn2y80v/nz6EAAG/RBAEA3qIJAgC8RRMEAHiLJggA8BZNEADgLZogAMBbNEEAgLdCPSwfBIE0yUCd5GBmlpKibWHatGlyTeWh38TpGPn5+bZ48WLnmsrKSnkP6hSFxsYrPr95Rb/97W/lrNnIeX32s5915sI8TKxerzCTcFasWCHl1q5dGz/OzMy0GTNmONeEmVDxzjvvSLkwD/KGmYBiNjI8QpkGE+b1VSfBhJmEE+ZhfTOzwcFB6V5vaGiQa+7bt0/KJQ4jcOnr65OzifV37tzpzEWjUbmmOnUrzP2lPPienp4eP+7q6rLXXnvNuSbMcBR1kpQyDWpUmIlAV8IvQQCAt2iCAABv0QQBAN6iCQIAvEUTBAB4iyYIAPAWTRAA4C2aIADAWzRBAIC3aIIAAG+FGps2NDRkTU1NzlyYsVLqmCJlXNuo7OxsZyZxnFYsFrOenh7nmkOHDsl7UMYomemjn8xMeu0TXb582S5cuODMtbS0yDXVMU0ZGRlyzenTp8vZUUlJSZaVleXMxWIxuaZ63w4ODso1wwqCQKpfUlIi1+zu7pZyYUahnT9/Xs6amfX29trf/va3q7qHs2fPSrnEUWAuNTU1Uq6uri5+nJycbPn5+c411dXV8j7UUW9hRrE99NBDzsy5c+fixwMDA9IYu6qqKnkP6rU4ffq0XPN/il+CAABv0QQBAN6iCQIAvEUTBAB4iyYIAPAWTRAA4C2aIADAWzRBAIC3aIIAAG+FmhijCjPd5eLFi1Ju06ZNcs2JEyc6M83NzfHjc+fO2cqVK51rTp06Je9BndIRZppFZmamnDUbuQ7K5InE18JFnZby/PPPyzXnzp0rZ0fFYjHpNVYmAY167733pFwkEpFrKpNz+vv748fqJJwjR47Ie1DfYydOnJBrTps2Tc6ajUwa2rx5szO3bds2uaYyGcpMfy+amRUWFsrZxH188pOfdOauv/56ueYLL7wg5dauXSvXnDp1qjOTOMGrqKjIPvOZzzjX1NbWyntQ32MpKXprOn78uJy9En4JAgC8RRMEAHiLJggA8BZNEADgLZogAMBbNEEAgLdoggAAb9EEAQDeogkCALxFEwQAeCvU2LSUlBQrLS115gYGBuSaQ0NDUu78+fNyzdWrVzsz0Wg0ftzV1WU7d+50rklK0r8zFBUVSbmqqiq5Zk1NjZQbHU2UkpIi7eOuu+6S93DgwAEpt3fvXrnmz3/+czk7Kjk52QoKCpy5MPdNa2urlEtLS5NrKntMvBfNRkbCuVRXV8t7qKiokHLXXHONXLO9vV3Omo2Mr3vzzTedOeXzZdSECROkXJgRXMposX9VXFxs9913nzPX2dkp15w/f76UUz8TzMxuuOEGZ6alpeV9tZ999lnnmqamJnkPr776qpQ7ffq0XFO9Fz9on/wSBAB4iyYIAPAWTRAA4C2aIADAWzRBAIC3aIIAAG/RBAEA3qIJAgC8RRMEAHgrEgSBHo5EWsys8cPbzn/VpCAISs3G3HmZ/fPcxup5mY25azZWz8uMe/GjZqyel1nCuSUK1QQBABhL+HMoAMBbNEEAgLdoggAAb9EEAQDeogkCALxFEwQAeIsmCADwFk0QAOAtmiAAwFs0QQCAt2iCAABv0QQBAN6iCQIAvEUTBAB4iyYIAPAWTRAA4C2aIADAWzRBAIC3aIIAAG+lhAkXFBQElZWVzlxmZqZcs6+vT8p1d3fLNS9duuTMDAwM2NDQUMTMLCcnJygqKnKuCYJA3kN/f7+UGx4elmumpqZKuebm5mgQBKWpqalBWlqaMz80NCTvQZWSot9aWVlZUi4ajUaDICg1M8vMzAzy8vKcawYHB+V9FBYWSjnlNR2Vk5PjzDQ0NFg0Go2YmaWnpwfKGvVeCEN9L5qNvH/EXDQIgtK0tLRAvc6qpCTtO3xxcbFcMz09XcodPXo0fi9mZWVJ92KY91lycrKUi8Vick3lc7m1tdW6u7sjZmb5+flBWVmZc01+fr68hxMnTki5MJ+1yh7NzN577734NUsUqglWVlbahg0bnLnp06fLNY8ePSrl9u7dK9dU9pj47xYVFdmqVauca8LcxOrFbmtrk2sqX0DMzJ566qlGs5EP6xkzZjjzLS0t8h5UpaX/3732gWbPni3l1q1b1zh6nJeXZ8uWLXOuaWhokPdx9913S7mqqiq55k033eTMzJs3L36ck5Njd955p3NNRUWFvAeV+l40Mzt9+rSUq6+vbzQb+aJz8803O/ORSETeg9pU7733Xrnm5MmTpdz06dPfdy8uX77cuSbM+6ygoEDKhflxMHPmTGfm8ccfjx+XlZXZk08+6Vxz1113yXu49dZbpVyYHwff/va3pdxXv/rVxiv9f/4cCgDwFk0QAOAtmiAAwFs0QQCAt2iCAABv0QQBAN6iCQIAvBXqOcHBwUE7d+6cM/fyyy/LNdXnuNavXy/XXLp0qTNz6tSp+HFxcbH0LNF3vvMdeQ8XLlyQckeOHJFrPvbYY3LWbORZG+U5xKamJrmm+mDsggUL5Jqf/vSnpdy6devix21tbfb8888716gPdZuZvfXWW1Ju8eLFck3lYf2urq74cU9Pjx08eNC5pr29Xd6DMgjCLNwzlWGeBTYbebA9OzvbmTt58qRcUx2EsH37drlmmGfeRvX29krX7J133pFrNjc3S7mFCxfKNZcsWeLMJA4LiEQi0vCARx99VN6D+mC7OizAzKy2tlbOXgm/BAEA3qIJAgC8RRMEAHiLJggA8BZNEADgLZogAMBbNEEAgLdoggAAb9EEAQDeogkCALwVamxaNBp93+iqD6KMVhvV29sr5ZSxZqPuvvtuZ+btt9+OH0ejUXvmmWecazZv3izvIRaLSbmvf/3rcs37779fyj3wwAPxPXR3dzvzfX198h5mzZol5ZRrMOo/GXuUnZ1tN954ozPX0dEh1wyCQMqdOXNGrvnII484M3//+9/jxwMDA1ZfX+9co+7VzGzmzJlSLsxrlZubK2fNzCorK+1HP/qRM/fwww/LNZVRZWZmx48fl2sqn2//Kj093a699trQ6/4ddSScem3NzFpaWpyZy5cvx4+DIJDGDn7/+9+X91BeXn5Vc2ZmW7dulbNXwi9BAIC3aIIAAG/RBAEA3qIJAgC8RRMEAHiLJggA8BZNEADgLZogAMBbNEEAgLdCTYwZHBx833SLD3LhwgW55s033yzlsrOz5Zo9PT3OTOJEl7a2NnvxxReda7q6uuQ9FBQUSLk9e/bINbdt2yZnR6WkhLrEThkZGVIuJydHrpmfnx96HxUVFfaDH/zAmQvz+r7wwgtSbteuXXJNZarNv05/SU5Odq4Jc13V17eyslKumZ6eLmfNzIaGhiwajTpzn/jEJ+Sa6qSjV199Va6ZlpYmZ0fFYjHr7+935ubNmyfXHBoaknJhpqX87ne/c2ba2trixy0tLfab3/zGuaaiokLew/XXXy/lEid6uXR2dsrZK+GXIADAWzRBAIC3aIIAAG/RBAEA3qIJAgC8RRMEAHiLJggA8BZNEADgLZogAMBbNEEAgLdCzdRKS0uziRMnOnPLly+Xa86ePVvKhRmbNn/+fGfmZz/7Wfy4qqrKnnjiCeea48ePy3s4dOiQlAszYq6lpUXOmplFIhFpBFdSkv5daP/+/VJu1apVcs2FCxfK2VFZWVk2Z84cZy7MqLt3331Xyn384x+Xa06bNs2ZefTRR+PHZWVl9pWvfMW5ZvLkyfIe1HFdNTU1cs3y8nIpF4lEzGxkzJqy51OnTsl7qK2tlXLKqMdRs2bNknJbtmyJH9fU1Nj69euda+rq6uR9HDlyRMpt3rxZrrlixQpnZt26dfHj5ORkaeTej3/8Y3kP6uf466+/Lte89957pVzi+ywRvwQBAN6iCQIAvEUTBAB4iyYIAPAWTRAA4C2aIADAWzRBAIC3aIIAAG/RBAEA3ooEQaCHI5EWM2v88LbzXzUpCIJSszF3Xmb/PLexel5mY+6ajdXzMuNe/KgZq+dllnBuiUI1QQAAxhL+HAoA8BZNEADgLZogAMBbNEEAgLdoggAAb9EEAQDeogkCALxFEwQAeIsmCADwFk0QAOAtmiAAwFs0QQCAt2iCAABv0QQBAN6iCQIAvEUTBAB4iyYIAPAWTRAA4C2aIADAWzRBAIC3UsKECwoKgsrKSmduYGBArtnR0SHlUlL0rebm5jozzc3N1tHRETEzS0tLC7KyspxrhoeH5T0UFBRc1VyYf//dd9+NBkFQWlJSEkyaNMmZj8Vi8h6i0ehVzZmZDQ0NqbloEASlZmbp6enSNQtzLwZBIOUuX74s11Re2yAILAiCiJlZcnJyoNzrkUhE3oPyfjAzy8/Pl2uq9+2hQ4eiQRCUZmdnB8qazMxMeQ/qfdvb2yvXVF+r+vr6+L2Ij7ZQTbCystLWr1/vzJ0+fVquuW3bNilXUlIi17zlllucmZUrV8aPs7KypDWdnZ3yHr7whS9IuSVLlsg129vbpdzcuXMbzcwmTZpkb7zxhjMfplGsW7dOyj3zzDNyTbVhXrx4sXH0OCsry26//Xbnmvr6enkfanO7cOGCXFN5bRMzKSkpVl5e7lyTnp4u70F5nczMFi1aJNdU7+9IJNJoNtI0v/WtbznzM2fOlPfQ1dUl5erq6uSayueAmdmSJUsa3Sl8FPDnUACAt2iCAABv0QQBAN6iCQIAvEUTBAB4iyYIAPAWTRAA4K1Qzwm2t7fbli1bnLl9+/bJNZOTk6VcT0+PXFN5qDzxwejh4WFra2tzrgnzAHhfX5+UGzdunFzzmmuukbNmIw9UK8+TPfXUU3LNPXv2SLmLFy/KNdUHrxNr9vf327Fjx5xrwtw36vVNS0uTa1ZVVTkzZ8+ejR8nJydLD60fOXJE3kNDQ4OUmzBhglyzoqJCzpqZFRUV2bJly5y5jRs3yjWVYQlm4QZtHD58WM5ibOCXIADAWzRBAIC3aIIAAG/RBAEA3qIJAgC8RRMEAHiLJggA8BZNEADgLZogAMBbNEEAgLdCjU3r6uqynTt3OnOpqalyzf7+fil3/PhxuaYy/qm7u/t9/52U5P4+EGYE14EDB6TcSy+9JNecMmWKnDUbGcf14IMPOnO7d++Wax49elTKZWZmyjXDjOsaNTw8bB0dHc7cP/7xD7nmvHnzpNycOXOuas3Vq1fHjwsLC+1LX/qSc83kyZPlPezfv1/K7dixQ6555swZOWs2MsJPGZEY5l7cvn27lKuurpZr3nLLLXIWYwO/BAEA3qIJAgC8RRMEAHiLJggA8BZNEADgLZogAMBbNEEAgLdoggAAb9EEAQDeCjUxJicnx2666SZn7sSJE3LNf53c8kEaGhrkmk888YScNTOLxWLW19cn5VR79uyRcnv37pVrjh8/Xs6amXV0dNgf//hHZ66xsVGuOW7cOCmXnZ0t1ywvL5ezo4aHh62rq8uZCzMtZPbs2VJOmegyatasWc7ML3/5y/hxRkaGXXfddc416qQlM7NLly5Juddff12uefjwYTlrZnb69GlbunSpM3fhwgW55tSpU6Xc4OCgXDPMpCOMDfwSBAB4iyYIAPAWTRAA4C2aIADAWzRBAIC3aIIAAG/RBAEA3qIJAgC8RRMEAHiLJggA8FaosWnjx4+3n/zkJ87cqVOn5JrJyclS7pvf/KZcc+bMmc7Mhg0b4sepqalWUVHhXJOUpH9naGlpkXIXL16Ua9bV1clZM7NIJGKpqanOXG1trVxzwoQJUk75d0dVVVXJ2cT6yri1adOmyTUXLVok5e644w65ZjQalbNmI/dYTk6OMxfmNVPubTOzIAjkmjU1NVJu9J5NSUmx0tJSZ37ZsmXyHh5++GEpF2Y0YV5enpT79a9/LdfE/238EgQAeIsmCADwFk0QAOAtmiAAwFs0QQCAt2iCAABv0QQBAN6iCQIAvEUTBAB4KxJmSkQkEmkxs8YPbzv/VZOCICg1G3PnZfbPcxur52U25q7ZWD0vMw/uRXy0hWqCAACMJfw5FADgLZogAMBbNEEAgLdoggAAb9EEAQDeogkCALxFEwQAeIsmCADwFk0QAOCt/wc1QCCX0DxNxwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 30 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CNN 시각화 하기 \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from simple_convnet import SimpleConvNet\n",
    "\n",
    "def filter_show(filters, nx=8, margin=3, scale=10):\n",
    "    \"\"\"\n",
    "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
    "    \"\"\"\n",
    "    FN, C, FH, FW = filters.shape\n",
    "    ny = int(np.ceil(FN / nx))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "    for i in range(FN):\n",
    "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
    "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "network = SimpleConvNet()\n",
    "# 무작위(랜덤) 초기화 후의 가중치\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 학습된 가중치\n",
    "network.load_params(\"params.pkl\")\n",
    "filter_show(network.params['W1'])\n",
    "\n",
    "# 위와 같이 학습 전 필터는 무작위로 초기화 되어 흑백 정도에 규칙성이 없다. \n",
    "# 학습 마친 필터는 규칙성 있는 이미지가 되었다. \n",
    "# 흰색에서 검은색으로 점차 변화하는 필터와 덩어리(블롭)가 진 필터 등, 규칙을 띄는 필터로 바뀜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• 딥러닝 흥미로운 점 \n",
    "\n",
    ": 합성곱 계층을 여러 겹 쌓으면, 층이 깊어지면서 더 복잡하고 추상화된 정보가 추출된다. \n",
    "####\n",
    "→ 처음 층은 단순한 에지에 반응, 이어 텍스처에 반응, 더 복잡한 사물의 일부에 반응하도록 변화 \n",
    "####\n",
    "▶︎ 즉, 층이 깊어지면서 뉴런이 반응하는 대상이 단순한 모양에서 '고급' 정보로 변화해간다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### • 대표적인 CNN\n",
    "\n",
    "#### 1. LeNet\n",
    "\n",
    ": 손글씨 숫자를 인식하는 네트워크 \n",
    "\n",
    "→ 활성화 함수 / 주로 ReLU 사용 / 최대 풀링이 주류 \n",
    "\n",
    "▶︎ 시그모이드를 사용하는 데 반해, ReLU로 사용하고 서브샘플링 해서 중간 데이터 크기 줄이는데 현재 최대 풀링 이용하는 것\n",
    "\n",
    "\n",
    "#### 2. AlexNet \n",
    "\n",
    ": 합성곱 계층과 풀링 계층을 거듭하며 마지막으로 완전연결 계층을 거쳐 결과를 출력 \n",
    "\n",
    "→ 활성화 함수로 ReLU 사용 / LRN이라는 국소적 정규화 실시하는 계층 이용 / 드롭아웃 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8장 정리\n",
    "\n",
    "#### • CNN \n",
    "\n",
    "- 지금까지 완전연결 계층 네트워크에 합성곱 계층과 풀링 계층 새로 추가한다.\n",
    "\n",
    "- 합성곱 계층과 풀링 계층은 im2col(이미지 행렬로 전개하는 함수) 이용하면 간단하고 효율적으로 구현 \n",
    "\n",
    "- CNN 시각화해보면 계층이 깊어질수록 고급 정보가 추출되는 모습 확인\n",
    "\n",
    "- LeNet, AlexNet이 있다.\n",
    "\n",
    "- 딥러닝 발전에는 빅데이터와 GPU가 크게 기여했다. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4685cf8fad66a3b5e1d83fe36d59de227ad64c6532a47c7a0b6e8e6da20ed48b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('krc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
