{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기울기\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열 생성\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]  # f(x+h) 계산\n",
    "\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        x[idx] = tmp_val - h   # f(x-h) 계산\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val  # 값 복원\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 0.]\n",
      "[0. 0.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 기울기는 각 지점에서 낮아지는 방향\n",
    "# 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def function_2(x):\n",
    "    return x[0] ** 2 + x[1] ** 2  # or return np.sum(x**2)\n",
    "\n",
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))  # [ 6.  8.]\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))  # [ 0.  4.]\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))  # [ 6.  0.]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 경사법(경사 하강법)\n",
    ": 기울기를 잘 이용해 함수의 최솟값(가능한 한 작은 값)을 찾으려는 것\n",
    "\n",
    "- 현 위치에서 기울어진 방향으로 일정 거리만큼 이동\n",
    "\n",
    "- 이동한 곳에서도 기울기 구하고, 기울어진 방향으로 나아가기 반복 → 함수의 값을 점차 줄이는 것\n",
    "\n",
    "### 학습률 \n",
    ": 한 번의 학습으로 얼만큼 학습해야 하는지, 즉 매개변수 값을 얼마나 갱신하느냐 정하는 것\n",
    "\n",
    "\n",
    "\n",
    "### 하이퍼파라미터\n",
    ": 학습률 같은 매개변수 (사람이 직접 설정하는 매개변수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법 구현\n",
    "# f: 최적화하려는 함수 / init_x : 초기값 / lr : learning rate / step_num : 경사법 따른 반복 횟수  \n",
    "\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num): # 기울기에 학습률 곱한 값으로 갱신하는 처리 step_num번 반복\n",
    "        grad = numerical_gradient(f, x) # 기울기\n",
    "        x -= lr * grad \n",
    "\n",
    "    return x    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.10953066e-10,  4.00000000e+00])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-25898425937328.36\n",
      "-2.999999939999995\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 큼 / lr = 10.0\n",
    "# [ -2.58983747e+13  -1.29524862e+12] 발산함\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=10.0)\n",
    "print(x)  \n",
    "\n",
    "# 학습률이 너무 작음 / lr = 1e-10\n",
    "# [-2.99999994  3.99999992] 갱신되지 않은 채 끝나서 거의 변화 없음\n",
    "\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=1e-10)\n",
    "print(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 간단한 신경망 구현\n",
    "import sys, os\n",
    "sys.path.append(\"/Users/krc/Downloads/deep-learning-from-scratch-master\")  # 부모 디렉터리의 파일을 가져올 수 있도록 설정\n",
    "import numpy as np\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 알고리즘 구현하기\n",
    "\n",
    "##### ► 신경망 학습 4단계\n",
    "\n",
    "1. 미니배치 \n",
    ": 훈련 데이터 중 일부 무작위로 가져옴. 선별한 데이터가 미니배치 → 손실 함수 값 줄이는 것이 목표\n",
    "\n",
    "\n",
    "2. 기울기 산출\n",
    ": 미니배치의 손실 함수 값 줄이기 위해 각 가중치 매개변수 기울기 구함. → 기울기는 손실 함수 값 가장 작게 하는 방향\n",
    "\n",
    "3. 매개변수 갱신 \n",
    ": 가중치 매개변수 기울기 방향으로 아주 조금 갱신\n",
    "\n",
    "4. 반복\n",
    ": 1~3단계 반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"/Users/krc/Downloads/deep-learning-from-scratch-master\")\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • loss(self, x, t) \n",
    ": 손실 함수의 값 계산하는 메서드 \n",
    "\n",
    "→ predict() 결과와 정답 레이블 바탕으로 교차 엔트로피 오차 구하도록 구현\n",
    "\n",
    "\n",
    "#### • numerical_gradient(self, x, t) \n",
    ": 각 매개변수의 기울기 계산\n",
    "\n",
    "→ 수치 미분 방식으로 각 매개변수의 손실 함수에 대한 기울기 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.09915, 0.1009\n",
      "train acc, test acc | 0.7760666666666667, 0.7815\n",
      "train acc, test acc | 0.8764, 0.8793\n",
      "train acc, test acc | 0.8986166666666666, 0.9016\n",
      "train acc, test acc | 0.9073666666666667, 0.9103\n",
      "train acc, test acc | 0.91365, 0.9151\n",
      "train acc, test acc | 0.9180333333333334, 0.9223\n",
      "train acc, test acc | 0.9224166666666667, 0.9259\n",
      "train acc, test acc | 0.9277, 0.9294\n",
      "train acc, test acc | 0.9303, 0.9318\n",
      "train acc, test acc | 0.9321666666666667, 0.9325\n",
      "train acc, test acc | 0.9358166666666666, 0.9352\n",
      "train acc, test acc | 0.93835, 0.9371\n",
      "train acc, test acc | 0.9404, 0.9404\n",
      "train acc, test acc | 0.94185, 0.9417\n",
      "train acc, test acc | 0.9447833333333333, 0.9434\n",
      "train acc, test acc | 0.94675, 0.9447\n"
     ]
    }
   ],
   "source": [
    "# 미니배치 학습 구현하기\n",
    "# 미니배치 학습 : 훈련 데이터 중 일부를 무작위로 꺼냄(미니배치), 미니배치에 대해 경사법으로 매개변수 갱신\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(\"/Users/krc/Downloads/deep-learning-from-scratch-master/ch04\")  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 데이터 읽기\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 매번 60,000개의 훈련 데이터에서 임의로 100개의 데이터(이미지 데이터, 정답 레이블 데이터) 추려냄\n",
    "\n",
    "##### 2. 100개의 미니배치 대상으로 확률적 경사 하강법 수행해 매개변수 갱신\n",
    "\n",
    "##### 3. 경사법에 의한 갱신 횟수(반복 횟수) 10,000번 설정\n",
    "\n",
    "##### 4. 갱신할 때마다 훈련 데이터에 대한 손실 함수 계산, 그 값을 배열에 추가\n",
    "\n",
    "→ 학습 횟수가 늘어나면서 손실 함수의 값 줄어든다. 신경망의 가중치 매개변수가 서서히 데이터에 적응하고 있음을 의미\n",
    "\n",
    "######\n",
    "★ 즉, 데이터를 반복해 학습함으로써 최적 가중치 매개변수로 다가가고 있음 ★"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프 그리기\n",
    "# 에폭이 진행될수록 정확도가 모두 좋아지고 있다. (두 선이 거의 겹쳐 있다.) → 오버피팅이 일어나지 않았다. \n",
    "\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### • 손실 함수의 값 \n",
    ": 훈련 데이터의 미니배치에 대한 손실 함수의 값\n",
    "\n",
    "\n",
    "#### • 오버피팅 \n",
    "\n",
    ": 훈련 데이터에 포함된 이미지만 제대로 구분하고, 그렇지 않은 이미지는 식별할 수 없다. \n",
    "\n",
    "\n",
    "#### • 에폭 \n",
    "\n",
    ": 하나의 단위\n",
    "\n",
    ": 1에폭은 학습에서 훈련 데이터를 모두 소진했을 때의 횟수 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정리 \n",
    "\n",
    "### 신경망 학습 \n",
    "- 손실 함수라는 '지표' 도입 : 가장 먼저 신경망이 학습을 수행할 수 있도록\n",
    "\n",
    "- 목표 : 손실 함수 기준으로 값이 가장 작아지는 가중치 매개변수 값 찾아내는 것\n",
    "\n",
    "- 경사법 : 함수의 기울기 이용하는 방법 / 가능한 한 작은 손실 함수의 값 찾는 수법"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4685cf8fad66a3b5e1d83fe36d59de227ad64c6532a47c7a0b6e8e6da20ed48b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('krc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
